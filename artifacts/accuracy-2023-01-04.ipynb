{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from CCS import CCS\n",
    "from IPython.display import display\n",
    "from LogisticRegression import LogisticRegression\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from jinja2 import Environment, PackageLoader, select_autoescape\n",
    "from pprint import pp\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import lightning as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "VERBOSE = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "login(add_to_git_credential=True)\n",
    "np_rand = np.random.default_rng(seed=100500)\n",
    "pp(device)\n",
    "model_type = torch.bfloat16\n",
    "\n",
    "pt_template = \"ggplot2\""
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a0fec903fdc4d8199c1971effdfafcc"
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device(type='cuda')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Load model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    # device_map=device,\n",
    ")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "hf_model = LlamaForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    torch_dtype=model_type,\n",
    "    device_map=device,\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfd99d98466f4b53a28bff4d017e3dfe"
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "402aa957300a4c368fabe8ec1e40c6ad"
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00a2d2da87834b7181b613414fd3c4e3"
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f7ac7adb05546b9a5f62cc7cb84e538"
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f666778ff2d434799d1a0e47c23729d"
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b66a49a701024bfd9a143e048f82bd17"
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73c14a4a5e904b1c872f761b7879085e"
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ef298dac83946f282a7c27a00ae0471"
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54dd593d7e334642a5c49969216bca96"
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c919c17b23c9417d9576272520405f55"
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b248e4d608844c1b2f935e984e0a85f"
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "hf_model.eval()\n",
    "pp(hf_model)\n",
    "pp(hf_model.config)\n",
    "with torch.no_grad():\n",
    "    pp(\n",
    "        tokenizer.batch_decode(\n",
    "            hf_model.generate(\n",
    "                tokenizer(\"The capital of Russia is\", return_tensors=\"pt\").input_ids.to(\n",
    "                    device\n",
    "                ),\n",
    "                max_length=20,\n",
    "            )\n",
    "        )[0]\n",
    "    )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "('<s> The capital of Russia is Moscow, located in the western part of the '\n",
      " 'country. It is the')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "true_ids = [5574, 5852, 1565, 3009]\n",
    "false_ids = [7700, 8824, 2089, 4541]\n",
    "print(\" \".join(f\"'{tokenizer.decode(id_)}'\" for id_ in true_ids))\n",
    "print(\" \".join(f\"'{tokenizer.decode(id_)}'\" for id_ in false_ids))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'True' 'True' 'true' 'true'\n",
      "'False' 'False' 'false' 'false'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "truthfulqa = load_dataset(\"truthful_qa\", \"generation\")  # 817 rows\n",
    "env = Environment(loader=PackageLoader(\"utils\"), autoescape=select_autoescape())"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7b88f03b4e7435aa585a59f2a0dea9a"
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/223k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eaa7cf80b1fa46049a282da90db98fdb"
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Datasets generators:\n",
    "\n",
    "def one_statement_ds_generator():\n",
    "    with torch.no_grad():\n",
    "        template = env.get_template(\"question_answer.jinja\")\n",
    "        counter = 0\n",
    "        for row_id, row in tqdm(list(enumerate(truthfulqa[\"validation\"]))):\n",
    "            inc_as = row['incorrect_answers']\n",
    "            cor_as = row['correct_answers']\n",
    "            for take_correct in (True, False):\n",
    "                for ans in (cor_as, inc_as)[take_correct]:\n",
    "                    counter += 1\n",
    "                    yield {\n",
    "                        'id': counter, \n",
    "                        'question_id':row_id,\n",
    "                        'row':row,\n",
    "                        'template_render_fn': template.render,\n",
    "                        'is_correct': take_correct,\n",
    "                        'template_render_params':{\n",
    "                            'question':row[\"question\"],\n",
    "                            'answer':ans\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "def conj_ds_generator():\n",
    "    with torch.no_grad():\n",
    "        template = env.get_template(\"question_answers.jinja\")\n",
    "        counter = 0\n",
    "        for row_id, row in tqdm(list(enumerate(truthfulqa[\"validation\"]))):\n",
    "            inc_as = row['incorrect_answers']\n",
    "            cor_as = row['correct_answers']\n",
    "            def create_yield(is_correct, answers):\n",
    "                nonlocal counter\n",
    "                counter += 1\n",
    "                return {\n",
    "                    'id': counter, \n",
    "                    'question_id':row_id,\n",
    "                    'row':row,\n",
    "                    'template_render_fn': template.render,\n",
    "                    'is_correct': is_correct,\n",
    "                    'template_render_params':{\n",
    "                        'question':row[\"question\"],\n",
    "                        'answers': answers,\n",
    "                        'is_disjunction':False\n",
    "                    }\n",
    "                }\n",
    "            # Take correct:\n",
    "            for c_a in cor_as:\n",
    "                other_c_a = [a for a in cor_as if a != c_a][0]\n",
    "                yield create_yield(True, [c_a, other_c_a])\n",
    "            # Take incorrect:\n",
    "            for i_a in inc_as:\n",
    "                for c_a in cor_as:\n",
    "                    yield create_yield(False, [i_a, c_a])\n",
    "\n",
    "def disj_ds_generator():\n",
    "    with torch.no_grad():\n",
    "        template = env.get_template(\"question_answers.jinja\")\n",
    "        counter = 0\n",
    "        for row_id, row in tqdm(list(enumerate(truthfulqa[\"validation\"]))):\n",
    "            inc_as = row['incorrect_answers']\n",
    "            cor_as = row['correct_answers']\n",
    "            def create_yield(is_correct, answers):\n",
    "                nonlocal counter\n",
    "                counter += 1\n",
    "                return {\n",
    "                    'id': counter, \n",
    "                    'question_id':row_id,\n",
    "                    'row':row,\n",
    "                    'template_render_fn': template.render,\n",
    "                    'is_correct': is_correct,\n",
    "                    'template_render_params':{\n",
    "                        'question':row[\"question\"],\n",
    "                        'answers': answers,\n",
    "                        'is_disjunction':True\n",
    "                    }\n",
    "                }\n",
    "            # Take correct:\n",
    "            for c_a in cor_as:\n",
    "                for i_a in inc_as:\n",
    "                    yield create_yield(True, [c_a, i_a])\n",
    "            for c_a in cor_as:\n",
    "                for other_c_a in [a for a in cor_as if a != c_a]:\n",
    "                    yield create_yield(True, [c_a, other_c_a])\n",
    "            # Take incorrect:\n",
    "            for i_a in inc_as:\n",
    "                other_i_a = [a for a in inc_as if a != i_a][0]\n",
    "                yield create_yield(False, [i_a, other_i_a])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "report = pd.DataFrame()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def calc_accuracy_for_one_statement():\n",
    "    known_questions = set()\n",
    "    wrong_questions = set()\n",
    "    count = 0\n",
    "    correct_n = 0\n",
    "    pbar = tqdm(one_statement_ds_generator())\n",
    "    for sample_gen in pbar:\n",
    "        input_ = sample_gen['template_render_fn'](**sample_gen['template_render_params'], label=\"\")\n",
    "        t_output = tokenizer(input_, return_tensors=\"pt\")\n",
    "        t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "        outputs = hf_model(**t_output, output_hidden_states=False)\n",
    "        pred = outputs.logits[0, -1].softmax(dim=-1)\n",
    "        token = pred.argmax(-1)\n",
    "        is_correct = token in true_ids if sample_gen['is_correct'] else token in false_ids\n",
    "        if is_correct:\n",
    "            correct_n += 1\n",
    "            if sample_gen['question_id'] not in wrong_questions:\n",
    "                known_questions.add(sample_gen['question_id'])\n",
    "        count += 1\n",
    "        pbar.set_description(\n",
    "            f\"Correct {correct_n}, count {count}, accuracy {correct_n / count:.4}, known {len(known_questions)}\"\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# FS - one statement\n",
    "\n",
    "fs_one_indexes, accuracy = calc_accuracy_for_one_statement()\n",
    "report.loc[\"One statement\", \"FS\"] = accuracy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 817/817 [05:07<00:00,  2.65it/s]724: : 5917it [05:07, 19.83it/s]\n",
      "Correct 2191, count 5918, accuracy 0.3702, known 724: : 5918it [05:07, 19.22it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# FS - one statement\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m fs_one_indexes, accuracy \u001b[39m=\u001b[39m calc_accuracy_for_one_statement()\n\u001b[1;32m      5\u001b[0m report\u001b[39m.\u001b[39mloc[\u001b[39m\"\u001b[39m\u001b[39mOne statement\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFS\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m accuracy\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def fs_calc_accuracy_for(ds_generator)\n",
    "    known_questions = set()\n",
    "    wrong_questions = set()\n",
    "    count = 0\n",
    "    correct_n = 0\n",
    "    pbar = tqdm(ds_generator())\n",
    "    for sample_gen in pbar:\n",
    "        input_ = sample_gen['template_render_fn'](**sample_gen['template_render_params'], label=\"\")\n",
    "        t_output = tokenizer(input_, return_tensors=\"pt\")\n",
    "        t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "        outputs = hf_model(**t_output, output_hidden_states=False)\n",
    "        pred = outputs.logits[0, -1].softmax(dim=-1)\n",
    "        token = pred.argmax(-1)\n",
    "        is_correct = token in true_ids if sample_gen['is_correct'] else token in false_ids\n",
    "        if is_correct:\n",
    "            correct_n += 1\n",
    "            if sample_gen['question_id'] not in wrong_questions:\n",
    "                known_questions.add(sample_gen['question_id'])\n",
    "        else:\n",
    "            wrong_questions.add(sample_gen['question_id'])\n",
    "            if sample_gen['question_id'] in known_questions:\n",
    "                known_questions.remove(sample_gen['question_id'])\n",
    "        count += 1\n",
    "        pbar.set_description(\n",
    "            f\"Correct {correct_n}, count {count}, accuracy {correct_n / count:.4}, known {len(known_questions)}\"\n",
    "        )\n",
    "    return known_questions, correct_n / count"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "expected ':' (<ipython-input-11-49b4ceb43929>, line 2)",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    def fs_calc_accuracy_for(ds_generator)\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# FS - one statement\n",
    "\n",
    "fs_one_ans_known_questions, accuracy = fs_calc_accuracy_for(one_statement_ds_generator)\n",
    "report.loc[\"One statement\", \"FS\"] = accuracy"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'fs_calc_accuracy_for' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/dlkworks/accuracy.py\u001b[0m in \u001b[0;36mline 4\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=203'>204</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=204'>205</a>\u001b[0m \u001b[39m# FS - one statement\u001b[39;00m\n\u001b[0;32m----> <a href='file:///workspace/dlkworks/accuracy.py?line=206'>207</a>\u001b[0m fs_one_ans_known_questions, accuracy \u001b[39m=\u001b[39m fs_calc_accuracy_for(one_statement_ds_generator)\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=207'>208</a>\u001b[0m report\u001b[39m.\u001b[39mloc[\u001b[39m\"\u001b[39m\u001b[39mOne statement\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFS\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m accuracy\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fs_calc_accuracy_for' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def fs_calc_accuracy_for(ds_generator):\n",
    "    known_questions = set()\n",
    "    wrong_questions = set()\n",
    "    count = 0\n",
    "    correct_n = 0\n",
    "    pbar = tqdm(ds_generator())\n",
    "    for sample_gen in pbar:\n",
    "        input_ = sample_gen['template_render_fn'](**sample_gen['template_render_params'], label=\"\")\n",
    "        t_output = tokenizer(input_, return_tensors=\"pt\")\n",
    "        t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "        outputs = hf_model(**t_output, output_hidden_states=False)\n",
    "        pred = outputs.logits[0, -1].softmax(dim=-1)\n",
    "        token = pred.argmax(-1)\n",
    "        is_correct = token in true_ids if sample_gen['is_correct'] else token in false_ids\n",
    "        if is_correct:\n",
    "            correct_n += 1\n",
    "            if sample_gen['question_id'] not in wrong_questions:\n",
    "                known_questions.add(sample_gen['question_id'])\n",
    "        else:\n",
    "            wrong_questions.add(sample_gen['question_id'])\n",
    "            if sample_gen['question_id'] in known_questions:\n",
    "                known_questions.remove(sample_gen['question_id'])\n",
    "        count += 1\n",
    "        pbar.set_description(\n",
    "            f\"Correct {correct_n}, count {count}, accuracy {correct_n / count:.4}, known {len(known_questions)}\"\n",
    "        )\n",
    "    return known_questions, correct_n / count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# FS - one statement\n",
    "\n",
    "fs_one_ans_known_questions, accuracy = fs_calc_accuracy_for(one_statement_ds_generator)\n",
    "report.loc[\"One statement\", \"FS\"] = accuracy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 817/817 [05:06<00:00,  2.67it/s]27: : 5916it [05:06, 20.04it/s]\n",
      "Correct 2191, count 5918, accuracy 0.3702, known 27: : 5918it [05:06, 19.31it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# FS - disjunction\n",
    "fs_disj_known_questions, accuracy = fs_calc_accuracy_for(disj_ds_generator)\n",
    "report.loc[\"Disjunction\", \"FS\"] = accuracy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  3%|▎         | 22/817 [00:51<30:57,  2.34s/it] : 928it [00:51, 17.08it/s] \n",
      "Correct 549, count 929, accuracy 0.591, known 1: : 929it [00:51, 18.03it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspace/dlkworks/accuracy.py\u001b[0m in \u001b[0;36mline 3\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=209'>210</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=210'>211</a>\u001b[0m \u001b[39m# FS - disjunction\u001b[39;00m\n\u001b[0;32m----> <a href='file:///workspace/dlkworks/accuracy.py?line=211'>212</a>\u001b[0m fs_disj_known_questions, accuracy \u001b[39m=\u001b[39m fs_calc_accuracy_for(disj_ds_generator)\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=212'>213</a>\u001b[0m report\u001b[39m.\u001b[39mloc[\u001b[39m\"\u001b[39m\u001b[39mDisjunction\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFS\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m accuracy\n",
      "\u001b[1;32m/workspace/dlkworks/accuracy.py\u001b[0m in \u001b[0;36mline 8\u001b[0m, in \u001b[0;36mfs_calc_accuracy_for\u001b[0;34m(ds_generator)\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=178'>179</a>\u001b[0m correct_n \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=179'>180</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(ds_generator())\n\u001b[0;32m----> <a href='file:///workspace/dlkworks/accuracy.py?line=180'>181</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample_gen \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=181'>182</a>\u001b[0m     input_ \u001b[39m=\u001b[39m sample_gen[\u001b[39m'\u001b[39m\u001b[39mtemplate_render_fn\u001b[39m\u001b[39m'\u001b[39m](\u001b[39m*\u001b[39m\u001b[39m*\u001b[39msample_gen[\u001b[39m'\u001b[39m\u001b[39mtemplate_render_params\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=182'>183</a>\u001b[0m     t_output \u001b[39m=\u001b[39m tokenizer(input_, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1178'>1179</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1180'>1181</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1181'>1182</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1182'>1183</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1183'>1184</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1184'>1185</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "\u001b[1;32m/workspace/dlkworks/accuracy.py\u001b[0m in \u001b[0;36mline 88\u001b[0m, in \u001b[0;36mdisj_ds_generator\u001b[0;34m()\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=164'>165</a>\u001b[0m \u001b[39m# Take incorrect:\u001b[39;00m\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=165'>166</a>\u001b[0m \u001b[39mfor\u001b[39;00m i_a \u001b[39min\u001b[39;00m inc_as:\n\u001b[0;32m---> <a href='file:///workspace/dlkworks/accuracy.py?line=166'>167</a>\u001b[0m     other_i_a \u001b[39m=\u001b[39m [a \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m inc_as \u001b[39mif\u001b[39;49;00m a \u001b[39m!=\u001b[39;49m i_a][\u001b[39m0\u001b[39;49m]\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=167'>168</a>\u001b[0m     \u001b[39myield\u001b[39;00m create_yield(\u001b[39mFalse\u001b[39;00m, [i_a, other_i_a])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# FS - conjunction\n",
    "fs_conj_known_questions, accuracy = fs_calc_accuracy_for(conj_ds_generator)\n",
    "report.loc[\"Conjunction\", \"FS\"] = accuracy"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "",
     "evalue": "",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Datasets generators:\n",
    "\n",
    "def one_statement_ds_generator():\n",
    "    with torch.no_grad():\n",
    "        template = env.get_template(\"question_answer.jinja\")\n",
    "        counter = 0\n",
    "        for row_id, row in tqdm(list(enumerate(truthfulqa[\"validation\"]))):\n",
    "            inc_as = row['incorrect_answers']\n",
    "            cor_as = row['correct_answers']\n",
    "            for take_correct in (True, False):\n",
    "                for ans in (cor_as, inc_as)[take_correct]:\n",
    "                    counter += 1\n",
    "                    yield {\n",
    "                        'id': counter, \n",
    "                        'question_id':row_id,\n",
    "                        'row':row,\n",
    "                        'template_render_fn': template.render,\n",
    "                        'is_correct': take_correct,\n",
    "                        'template_render_params':{\n",
    "                            'question':row[\"question\"],\n",
    "                            'answer':ans\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "def conj_ds_generator():\n",
    "    with torch.no_grad():\n",
    "        template = env.get_template(\"question_answers.jinja\")\n",
    "        counter = 0\n",
    "        for row_id, row in tqdm(list(enumerate(truthfulqa[\"validation\"]))):\n",
    "            inc_as = row['incorrect_answers']\n",
    "            cor_as = row['correct_answers']\n",
    "            def create_yield(is_correct, answers):\n",
    "                nonlocal counter\n",
    "                counter += 1\n",
    "                return {\n",
    "                    'id': counter, \n",
    "                    'question_id':row_id,\n",
    "                    'row':row,\n",
    "                    'template_render_fn': template.render,\n",
    "                    'is_correct': is_correct,\n",
    "                    'template_render_params':{\n",
    "                        'question':row[\"question\"],\n",
    "                        'answers': answers,\n",
    "                        'is_disjunction':False\n",
    "                    }\n",
    "                }\n",
    "            # Take correct:\n",
    "            for c_a in cor_as:\n",
    "                for other_c_a in [a for a in cor_as if a != c_a][0]:\n",
    "                    yield create_yield(True, [c_a, other_c_a])\n",
    "            # Take incorrect:\n",
    "            for i_a in inc_as:\n",
    "                for c_a in cor_as:\n",
    "                    yield create_yield(False, [i_a, c_a])\n",
    "\n",
    "def disj_ds_generator():\n",
    "    with torch.no_grad():\n",
    "        template = env.get_template(\"question_answers.jinja\")\n",
    "        counter = 0\n",
    "        for row_id, row in tqdm(list(enumerate(truthfulqa[\"validation\"]))):\n",
    "            inc_as = row['incorrect_answers']\n",
    "            cor_as = row['correct_answers']\n",
    "            def create_yield(is_correct, answers):\n",
    "                nonlocal counter\n",
    "                counter += 1\n",
    "                return {\n",
    "                    'id': counter, \n",
    "                    'question_id':row_id,\n",
    "                    'row':row,\n",
    "                    'template_render_fn': template.render,\n",
    "                    'is_correct': is_correct,\n",
    "                    'template_render_params':{\n",
    "                        'question':row[\"question\"],\n",
    "                        'answers': answers,\n",
    "                        'is_disjunction':True\n",
    "                    }\n",
    "                }\n",
    "            # Take correct:\n",
    "            for c_a in cor_as:\n",
    "                for i_a in inc_as:\n",
    "                    yield create_yield(True, [c_a, i_a])\n",
    "            for c_a in cor_as:\n",
    "                for other_c_a in [a for a in cor_as if a != c_a]:\n",
    "                    yield create_yield(True, [c_a, other_c_a])\n",
    "            # Take incorrect:\n",
    "            for i_a in inc_as:\n",
    "                for other_i_a in [a for a in inc_as if a != i_a][0]:\n",
    "                    yield create_yield(False, [i_a, other_i_a])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def fs_calc_accuracy_for(ds_generator):\n",
    "    known_questions = set()\n",
    "    wrong_questions = set()\n",
    "    count = 0\n",
    "    correct_n = 0\n",
    "    pbar = tqdm(ds_generator())\n",
    "    for sample_gen in pbar:\n",
    "        input_ = sample_gen['template_render_fn'](**sample_gen['template_render_params'], label=\"\")\n",
    "        t_output = tokenizer(input_, return_tensors=\"pt\")\n",
    "        t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "        outputs = hf_model(**t_output, output_hidden_states=False)\n",
    "        pred = outputs.logits[0, -1].softmax(dim=-1)\n",
    "        token = pred.argmax(-1)\n",
    "        is_correct = token in true_ids if sample_gen['is_correct'] else token in false_ids\n",
    "        if is_correct:\n",
    "            correct_n += 1\n",
    "            if sample_gen['question_id'] not in wrong_questions:\n",
    "                known_questions.add(sample_gen['question_id'])\n",
    "        else:\n",
    "            wrong_questions.add(sample_gen['question_id'])\n",
    "            if sample_gen['question_id'] in known_questions:\n",
    "                known_questions.remove(sample_gen['question_id'])\n",
    "        count += 1\n",
    "        pbar.set_description(\n",
    "            f\"Correct {correct_n}, count {count}, accuracy {correct_n / count:.4}, known {len(known_questions)}\"\n",
    "        )\n",
    "    return known_questions, correct_n / count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# FS - disjunction\n",
    "fs_disj_known_questions, accuracy = fs_calc_accuracy_for(disj_ds_generator)\n",
    "report.loc[\"Disjunction\", \"FS\"] = accuracy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  3%|▎         | 22/817 [05:31<3:19:23, 15.05s/it]: : 6121it [05:31, 17.43it/s]\n",
      "Correct 4358, count 6122, accuracy 0.7119, known 1: : 6122it [05:31, 18.48it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspace/dlkworks/accuracy.py\u001b[0m in \u001b[0;36mline 3\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=209'>210</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=210'>211</a>\u001b[0m \u001b[39m# FS - disjunction\u001b[39;00m\n\u001b[0;32m----> <a href='file:///workspace/dlkworks/accuracy.py?line=211'>212</a>\u001b[0m fs_disj_known_questions, accuracy \u001b[39m=\u001b[39m fs_calc_accuracy_for(disj_ds_generator)\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=212'>213</a>\u001b[0m report\u001b[39m.\u001b[39mloc[\u001b[39m\"\u001b[39m\u001b[39mDisjunction\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFS\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m accuracy\n",
      "\u001b[1;32m/workspace/dlkworks/accuracy.py\u001b[0m in \u001b[0;36mline 8\u001b[0m, in \u001b[0;36mfs_calc_accuracy_for\u001b[0;34m(ds_generator)\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=178'>179</a>\u001b[0m correct_n \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=179'>180</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(ds_generator())\n\u001b[0;32m----> <a href='file:///workspace/dlkworks/accuracy.py?line=180'>181</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample_gen \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=181'>182</a>\u001b[0m     input_ \u001b[39m=\u001b[39m sample_gen[\u001b[39m'\u001b[39m\u001b[39mtemplate_render_fn\u001b[39m\u001b[39m'\u001b[39m](\u001b[39m*\u001b[39m\u001b[39m*\u001b[39msample_gen[\u001b[39m'\u001b[39m\u001b[39mtemplate_render_params\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=182'>183</a>\u001b[0m     t_output \u001b[39m=\u001b[39m tokenizer(input_, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1178'>1179</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1180'>1181</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1181'>1182</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1182'>1183</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1183'>1184</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/tqdm/std.py?line=1184'>1185</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "\u001b[1;32m/workspace/dlkworks/accuracy.py\u001b[0m in \u001b[0;36mline 88\u001b[0m, in \u001b[0;36mdisj_ds_generator\u001b[0;34m()\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=164'>165</a>\u001b[0m \u001b[39m# Take incorrect:\u001b[39;00m\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=165'>166</a>\u001b[0m \u001b[39mfor\u001b[39;00m i_a \u001b[39min\u001b[39;00m inc_as:\n\u001b[0;32m---> <a href='file:///workspace/dlkworks/accuracy.py?line=166'>167</a>\u001b[0m     \u001b[39mfor\u001b[39;00m other_i_a \u001b[39min\u001b[39;00m [a \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m inc_as \u001b[39mif\u001b[39;49;00m a \u001b[39m!=\u001b[39;49m i_a][\u001b[39m0\u001b[39;49m]:\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=167'>168</a>\u001b[0m         \u001b[39myield\u001b[39;00m create_yield(\u001b[39mFalse\u001b[39;00m, [i_a, other_i_a])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# FS - conjunction\n",
    "fs_conj_known_questions, accuracy = fs_calc_accuracy_for(conj_ds_generator)\n",
    "report.loc[\"Conjunction\", \"FS\"] = accuracy"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "",
     "evalue": "",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Intersections of known questions:\n",
    "report.loc[\"Conjunction\", \"FS on known\"] = len(\n",
    "    set(fs_conj_known_questions).intersection(fs_one_ans_known_questions)\n",
    ") / len(fs_one_ans_known_questions)\n",
    "report.loc[\"Disjunction\", \"FS on known\"] = len(\n",
    "    set(fs_disj_known_questions).intersection(fs_one_ans_known_questions)\n",
    ") / len(fs_one_ans_known_questions)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "",
     "evalue": "",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Datasets generators:\n",
    "\n",
    "def one_statement_ds_generator():\n",
    "    with torch.no_grad():\n",
    "        template = env.get_template(\"question_answer.jinja\")\n",
    "        counter = 0\n",
    "        for row_id, row in tqdm(list(enumerate(truthfulqa[\"validation\"]))):\n",
    "            inc_as = row['incorrect_answers']\n",
    "            cor_as = row['correct_answers']\n",
    "            for take_correct in (True, False):\n",
    "                for ans in (cor_as, inc_as)[take_correct]:\n",
    "                    counter += 1\n",
    "                    yield {\n",
    "                        'id': counter, \n",
    "                        'question_id':row_id,\n",
    "                        'row':row,\n",
    "                        'template_render_fn': template.render,\n",
    "                        'is_correct': take_correct,\n",
    "                        'template_render_params':{\n",
    "                            'question':row[\"question\"],\n",
    "                            'answer':ans\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "def conj_ds_generator():\n",
    "    with torch.no_grad():\n",
    "        template = env.get_template(\"question_answers.jinja\")\n",
    "        counter = 0\n",
    "        for row_id, row in tqdm(list(enumerate(truthfulqa[\"validation\"]))):\n",
    "            inc_as = row['incorrect_answers']\n",
    "            cor_as = row['correct_answers']\n",
    "            def create_yield(is_correct, answers):\n",
    "                nonlocal counter\n",
    "                counter += 1\n",
    "                return {\n",
    "                    'id': counter, \n",
    "                    'question_id':row_id,\n",
    "                    'row':row,\n",
    "                    'template_render_fn': template.render,\n",
    "                    'is_correct': is_correct,\n",
    "                    'template_render_params':{\n",
    "                        'question':row[\"question\"],\n",
    "                        'answers': answers,\n",
    "                        'is_disjunction':False\n",
    "                    }\n",
    "                }\n",
    "            # Take correct:\n",
    "            for c_a in cor_as:\n",
    "                for other_c_a in [a for a in cor_as if a != c_a]:\n",
    "                    yield create_yield(True, [c_a, other_c_a])\n",
    "            # Take incorrect:\n",
    "            for i_a in inc_as:\n",
    "                for c_a in cor_as:\n",
    "                    yield create_yield(False, [i_a, c_a])\n",
    "\n",
    "def disj_ds_generator():\n",
    "    with torch.no_grad():\n",
    "        template = env.get_template(\"question_answers.jinja\")\n",
    "        counter = 0\n",
    "        for row_id, row in tqdm(list(enumerate(truthfulqa[\"validation\"]))):\n",
    "            inc_as = row['incorrect_answers']\n",
    "            cor_as = row['correct_answers']\n",
    "            def create_yield(is_correct, answers):\n",
    "                nonlocal counter\n",
    "                counter += 1\n",
    "                return {\n",
    "                    'id': counter, \n",
    "                    'question_id':row_id,\n",
    "                    'row':row,\n",
    "                    'template_render_fn': template.render,\n",
    "                    'is_correct': is_correct,\n",
    "                    'template_render_params':{\n",
    "                        'question':row[\"question\"],\n",
    "                        'answers': answers,\n",
    "                        'is_disjunction':True\n",
    "                    }\n",
    "                }\n",
    "            # Take correct:\n",
    "            for c_a in cor_as:\n",
    "                for i_a in inc_as:\n",
    "                    yield create_yield(True, [c_a, i_a])\n",
    "            for c_a in cor_as:\n",
    "                for other_c_a in [a for a in cor_as if a != c_a]:\n",
    "                    yield create_yield(True, [c_a, other_c_a])\n",
    "            # Take incorrect:\n",
    "            for i_a in inc_as:\n",
    "                for other_i_a in [a for a in inc_as if a != i_a]:\n",
    "                    yield create_yield(False, [i_a, other_i_a])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# FS - disjunction\n",
    "fs_disj_known_questions, accuracy = fs_calc_accuracy_for(disj_ds_generator)\n",
    "report.loc[\"Disjunction\", \"FS\"] = accuracy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 817/817 [28:32<00:00,  2.10s/it]n 39: : 30954it [28:32, 17.36it/s]\n",
      "Correct 17569, count 30954, accuracy 0.5676, known 39: : 30954it [28:32, 18.08it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# FS - conjunction\n",
    "fs_conj_known_questions, accuracy = fs_calc_accuracy_for(conj_ds_generator)\n",
    "report.loc[\"Conjunction\", \"FS\"] = accuracy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Correct 3328, count 4977, accuracy 0.6687, known 21: : 4977it [04:34, 18.14it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/dlkworks/accuracy.py\u001b[0m in \u001b[0;36mline 3\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=214'>215</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=215'>216</a>\u001b[0m \u001b[39m# FS - conjunction\u001b[39;00m\n\u001b[0;32m----> <a href='file:///workspace/dlkworks/accuracy.py?line=216'>217</a>\u001b[0m fs_conj_known_questions, accuracy \u001b[39m=\u001b[39m fs_calc_accuracy_for(conj_ds_generator)\n\u001b[1;32m      <a href='file:///workspace/dlkworks/accuracy.py?line=217'>218</a>\u001b[0m report\u001b[39m.\u001b[39mloc[\u001b[39m\"\u001b[39m\u001b[39mConjunction\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFS\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m accuracy\n",
      "\u001b[1;32m/workspace/dlkworks/accuracy.py\u001b[0m in \u001b[0;36mline 12\u001b[0m, in \u001b[0;36mfs_calc_accuracy_for\u001b[0;34m(ds_generator)\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=182'>183</a>\u001b[0m t_output \u001b[39m=\u001b[39m tokenizer(input_, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=183'>184</a>\u001b[0m t_output \u001b[39m=\u001b[39m {k: t_output[k]\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m t_output}\n\u001b[0;32m---> <a href='file:///workspace/dlkworks/accuracy.py?line=184'>185</a>\u001b[0m outputs \u001b[39m=\u001b[39m hf_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mt_output, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=185'>186</a>\u001b[0m pred \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='file:///workspace/dlkworks/accuracy.py?line=186'>187</a>\u001b[0m token \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1181\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1177'>1178</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1179'>1180</a>\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1180'>1181</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1181'>1182</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1182'>1183</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1183'>1184</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1184'>1185</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1185'>1186</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1186'>1187</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1187'>1188</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1188'>1189</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1189'>1190</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1190'>1191</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1192'>1193</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1193'>1194</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1068\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1057'>1058</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1058'>1059</a>\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1059'>1060</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1064'>1065</a>\u001b[0m         use_cache,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1065'>1066</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1066'>1067</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1067'>1068</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1068'>1069</a>\u001b[0m         hidden_states,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1069'>1070</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1070'>1071</a>\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1071'>1072</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1072'>1073</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1073'>1074</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1074'>1075</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1076'>1077</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=1078'>1079</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:809\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=806'>807</a>\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n\u001b[1;32m    <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=807'>808</a>\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=808'>809</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=809'>810</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py?line=810'>811</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///root/micromamba/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Intersections of known questions:\n",
    "report.loc[\"Conjunction\", \"FS on known\"] = len(\n",
    "    set(fs_conj_known_questions).intersection(fs_one_ans_known_questions)\n",
    ") / len(fs_one_ans_known_questions)\n",
    "report.loc[\"Disjunction\", \"FS on known\"] = len(\n",
    "    set(fs_disj_known_questions).intersection(fs_one_ans_known_questions)\n",
    ") / len(fs_one_ans_known_questions)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "",
     "evalue": "",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "fs_one_ans_known_questions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{9,\n",
       " 24,\n",
       " 99,\n",
       " 156,\n",
       " 173,\n",
       " 187,\n",
       " 265,\n",
       " 362,\n",
       " 370,\n",
       " 423,\n",
       " 425,\n",
       " 427,\n",
       " 428,\n",
       " 434,\n",
       " 438,\n",
       " 441,\n",
       " 475,\n",
       " 516,\n",
       " 559,\n",
       " 561,\n",
       " 562,\n",
       " 565,\n",
       " 663,\n",
       " 667,\n",
       " 683,\n",
       " 740,\n",
       " 809}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "fs_disj_known_questions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{22,\n",
       " 27,\n",
       " 28,\n",
       " 38,\n",
       " 70,\n",
       " 137,\n",
       " 141,\n",
       " 143,\n",
       " 165,\n",
       " 188,\n",
       " 197,\n",
       " 198,\n",
       " 225,\n",
       " 233,\n",
       " 248,\n",
       " 250,\n",
       " 251,\n",
       " 260,\n",
       " 265,\n",
       " 269,\n",
       " 385,\n",
       " 513,\n",
       " 604,\n",
       " 624,\n",
       " 643,\n",
       " 683,\n",
       " 686,\n",
       " 698,\n",
       " 717,\n",
       " 729,\n",
       " 733,\n",
       " 735,\n",
       " 737,\n",
       " 743,\n",
       " 747,\n",
       " 754,\n",
       " 771,\n",
       " 772,\n",
       " 809}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}