{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement CCS from scratch.\n",
    "This will deliberately be a simple (but less efficient) implementation to make everything as clear as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset amazon_polarity (/home/art/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n",
      "100%|██████████| 2/2 [00:00<00:00, 15.07it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m model_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdeberta\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     24\u001b[0m     model_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmicrosoft/deberta-v2-xxlarge\u001b[39;49m\u001b[39m\"\u001b[39;49m, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n\u001b[1;32m     26\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForMaskedLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/deberta-v2-xxlarge\u001b[39m\u001b[39m\"\u001b[39m, cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[1;32m     27\u001b[0m     model\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dlk-mG9MNTAf/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:709\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    708\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 709\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    710\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    711\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dlk-mG9MNTAf/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1825\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1823\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1825\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1826\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1827\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1828\u001b[0m     init_configuration,\n\u001b[1;32m   1829\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1830\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1831\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1832\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1833\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1834\u001b[0m     _is_local\u001b[39m=\u001b[39;49mis_local,\n\u001b[1;32m   1835\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1836\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dlk-mG9MNTAf/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1988\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1986\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1988\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1989\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1990\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1991\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1992\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1993\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dlk-mG9MNTAf/lib/python3.8/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py:133\u001b[0m, in \u001b[0;36mDebertaV2TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    120\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    132\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    134\u001b[0m         vocab_file,\n\u001b[1;32m    135\u001b[0m         tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    136\u001b[0m         do_lower_case\u001b[39m=\u001b[39;49mdo_lower_case,\n\u001b[1;32m    137\u001b[0m         bos_token\u001b[39m=\u001b[39;49mbos_token,\n\u001b[1;32m    138\u001b[0m         eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    139\u001b[0m         unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    140\u001b[0m         sep_token\u001b[39m=\u001b[39;49msep_token,\n\u001b[1;32m    141\u001b[0m         pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    142\u001b[0m         cls_token\u001b[39m=\u001b[39;49mcls_token,\n\u001b[1;32m    143\u001b[0m         mask_token\u001b[39m=\u001b[39;49mmask_token,\n\u001b[1;32m    144\u001b[0m         split_by_punct\u001b[39m=\u001b[39;49msplit_by_punct,\n\u001b[1;32m    145\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case \u001b[39m=\u001b[39m do_lower_case\n\u001b[1;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_by_punct \u001b[39m=\u001b[39m split_by_punct\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dlk-mG9MNTAf/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Let's just try IMDB for simplicity\n",
    "data = load_dataset(\"amazon_polarity\")[\"test\"]\n",
    "\n",
    "# Here are a few different model options you can play around with:\n",
    "model_name = \"deberta\"\n",
    "# model_name = \"gpt-j\"\n",
    "# model_name = \"t5\"\n",
    "\n",
    "# if you want to cache the model weights somewhere, you can specify that here\n",
    "cache_dir = None\n",
    "\n",
    "if model_name == \"deberta\":\n",
    "    model_type = \"encoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xxlarge\", cache_dir=cache_dir)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"microsoft/deberta-v2-xxlarge\", cache_dir=cache_dir)\n",
    "    model.cuda()\n",
    "elif model_name == \"gpt-j\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\", cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", cache_dir=cache_dir)\n",
    "    model.cuda()\n",
    "elif model_name == \"t5\":\n",
    "    model_type = \"encoder_decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-11b\", cache_dir=cache_dir)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-11b\", cache_dir=cache_dir)\n",
    "    model.parallelize()  # T5 is big enough that we may need to run it on multiple GPUs\n",
    "else:\n",
    "    print(\"Not implemented!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's write code for extracting hidden states given a model and text. \n",
    "How we do this exactly will depend on the type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    \n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_encoder_decoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    decoder_text_ids = tokenizer(\"\", return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, decoder_input_ids=decoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"encoder_hidden_states\"]\n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_decoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given a decoder model and some text, gets the hidden states (in a given layer, by default the last) on that input text\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize (adding the EOS token this time)\n",
    "    input_ids = tokenizer(input_text + tokenizer.eos_token, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_hidden_states(model, tokenizer, input_text, layer=-1, model_type=\"encoder\"):\n",
    "    fn = {\"encoder\": get_encoder_hidden_states, \"encoder_decoder\": get_encoder_decoder_hidden_states,\n",
    "          \"decoder\": get_decoder_hidden_states}[model_type]\n",
    "\n",
    "    return fn(model, tokenizer, input_text, layer=layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's write code for formatting data and for getting all the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_imdb(text, label):\n",
    "    \"\"\"\n",
    "    Given an imdb example (\"text\") and corresponding label (0 for negative, or 1 for positive), \n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "    \n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"The following movie review expresses a \" + [\"negative\", \"positive\"][label] + \" sentiment:\\n\" + text\n",
    "\n",
    "\n",
    "def get_hidden_states_many_examples(model, tokenizer, data, model_type, n=100):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples.\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    \n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "\n",
    "    # loop\n",
    "    for _ in tqdm(range(n)):\n",
    "        # for simplicity, sample a random example until we find one that's a reasonable length\n",
    "        # (most examples should be a reasonable length, so this is just to make sure)\n",
    "        while True:\n",
    "            idx = np.random.randint(len(data))\n",
    "            text, true_label = data[idx][\"content\"], data[idx][\"label\"]\n",
    "            # the actual formatted input will be longer, so include a bit of a marign\n",
    "            if len(tokenizer(text)) < 400:  \n",
    "                break\n",
    "                \n",
    "        # get hidden states\n",
    "        neg_hs = get_hidden_states(model, tokenizer, format_imdb(text, 0), model_type=model_type)\n",
    "        pos_hs = get_hidden_states(model, tokenizer, format_imdb(text, 1), model_type=model_type)\n",
    "\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "        all_gt_labels.append(true_label)\n",
    "\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "    all_gt_labels = np.stack(all_gt_labels)\n",
    "\n",
    "    return all_neg_hs, all_pos_hs, all_gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:19<00:00,  5.13it/s]\n"
     ]
    }
   ],
   "source": [
    "neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, data, model_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's verify that the model's representations are good\n",
    "\n",
    "Before trying CCS, let's make sure there exists a direction that classifies examples as true vs false with high accuracy; if logistic regression accuracy is bad, there's no hope of CCS doing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "# let's create a simple 50/50 train split (the data is already randomized)\n",
    "n = len(y)\n",
    "neg_hs_train, neg_hs_test = neg_hs[:n//2], neg_hs[n//2:]\n",
    "pos_hs_train, pos_hs_test = pos_hs[:n//2], pos_hs[n//2:]\n",
    "y_train, y_test = y[:n//2], y[n//2:]\n",
    "\n",
    "# for simplicity we can just take the difference between positive and negative hidden states\n",
    "# (concatenating also works fine)\n",
    "x_train = neg_hs_train - pos_hs_train\n",
    "x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(x_train, y_train)\n",
    "print(\"Logistic regression accuracy: {}\".format(lr.score(x_test, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try CCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPProbe(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d, 100)\n",
    "        self.linear2 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.linear1(x))\n",
    "        o = self.linear2(h)\n",
    "        return torch.sigmoid(o)\n",
    "\n",
    "class CCS(object):\n",
    "    def __init__(self, x0, x1, nepochs=1000, ntries=10, lr=1e-3, batch_size=-1, \n",
    "                 verbose=False, device=\"cuda\", linear=True, weight_decay=0.01, var_normalize=False):\n",
    "        # data\n",
    "        self.var_normalize = var_normalize\n",
    "        self.x0 = self.normalize(x0)\n",
    "        self.x1 = self.normalize(x1)\n",
    "        self.d = self.x0.shape[-1]\n",
    "\n",
    "        # training\n",
    "        self.nepochs = nepochs\n",
    "        self.ntries = ntries\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # probe\n",
    "        self.linear = linear\n",
    "        self.initialize_probe()\n",
    "        self.best_probe = copy.deepcopy(self.probe)\n",
    "\n",
    "        \n",
    "    def initialize_probe(self):\n",
    "        if self.linear:\n",
    "            self.probe = nn.Sequential(nn.Linear(self.d, 1), nn.Sigmoid())\n",
    "        else:\n",
    "            self.probe = MLPProbe(self.d)\n",
    "        self.probe.to(self.device)    \n",
    "\n",
    "\n",
    "    def normalize(self, x):\n",
    "        \"\"\"\n",
    "        Mean-normalizes the data x (of shape (n, d))\n",
    "        If self.var_normalize, also divides by the standard deviation\n",
    "        \"\"\"\n",
    "        normalized_x = x - x.mean(axis=0, keepdims=True)\n",
    "        if self.var_normalize:\n",
    "            normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
    "\n",
    "        return normalized_x\n",
    "\n",
    "        \n",
    "    def get_tensor_data(self):\n",
    "        \"\"\"\n",
    "        Returns x0, x1 as appropriate tensors (rather than np arrays)\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.x0, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.x1, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        return x0, x1\n",
    "    \n",
    "\n",
    "    def get_loss(self, p0, p1):\n",
    "        \"\"\"\n",
    "        Returns the CCS loss for two probabilities each of shape (n,1) or (n,)\n",
    "        \"\"\"\n",
    "        informative_loss = (torch.min(p0, p1)**2).mean(0)\n",
    "        consistent_loss = ((p0 - (1-p1))**2).mean(0)\n",
    "        return informative_loss + consistent_loss\n",
    "\n",
    "\n",
    "    def get_acc(self, x0_test, x1_test, y_test):\n",
    "        \"\"\"\n",
    "        Computes accuracy for the current parameters on the given test inputs\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.normalize(x0_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.normalize(x1_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            p0, p1 = self.best_probe(x0), self.best_probe(x1)\n",
    "        avg_confidence = 0.5*(p0 + (1-p1))\n",
    "        predictions = (avg_confidence.detach().cpu().numpy() < 0.5).astype(int)[:, 0]\n",
    "        acc = (predictions == y_test).mean()\n",
    "        acc = max(acc, 1 - acc)\n",
    "\n",
    "        return acc\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Does a single training run of nepochs epochs\n",
    "        \"\"\"\n",
    "        x0, x1 = self.get_tensor_data()\n",
    "        permutation = torch.randperm(len(x0))\n",
    "        x0, x1 = x0[permutation], x1[permutation]\n",
    "        \n",
    "        # set up optimizer\n",
    "        optimizer = torch.optim.AdamW(self.probe.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "        batch_size = len(x0) if self.batch_size == -1 else self.batch_size\n",
    "        nbatches = len(x0) // batch_size\n",
    "\n",
    "        # Start training (full batch)\n",
    "        for epoch in range(self.nepochs):\n",
    "            for j in range(nbatches):\n",
    "                x0_batch = x0[j*batch_size:(j+1)*batch_size]\n",
    "                x1_batch = x1[j*batch_size:(j+1)*batch_size]\n",
    "            \n",
    "                # probe\n",
    "                p0, p1 = self.probe(x0_batch), self.probe(x1_batch)\n",
    "\n",
    "                # get the corresponding loss\n",
    "                loss = self.get_loss(p0, p1)\n",
    "\n",
    "                # update the parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return loss.detach().cpu().item()\n",
    "    \n",
    "    def repeated_train(self):\n",
    "        best_loss = np.inf\n",
    "        for train_num in range(self.ntries):\n",
    "            self.initialize_probe()\n",
    "            loss = self.train()\n",
    "            if loss < best_loss:\n",
    "                self.best_probe = copy.deepcopy(self.probe)\n",
    "                best_loss = loss\n",
    "\n",
    "        return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCS accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Train CCS without any labels\n",
    "ccs = CCS(neg_hs_train, pos_hs_train)\n",
    "ccs.repeated_train()\n",
    "\n",
    "# Evaluate\n",
    "ccs_acc = ccs.get_acc(neg_hs_test, pos_hs_test, y_test)\n",
    "print(\"CCS accuracy: {}\".format(ccs_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b80286374679f2ad472c61c83fc267d31329b5dea8e2dcaccb727123767724c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
