{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, GPT2Model, GPT2Tokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pprint import pp\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "import elk \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "gpt2_xl : GPT2Model = GPT2Model.from_pretrained('gpt2-xl')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "gpt2_xl.eval()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 1600)\n",
       "  (wpe): Embedding(1024, 1600)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-47): 48 x GPT2Block(\n",
       "      (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "truthfulqa = load_dataset('truthful_qa', 'generation')\n",
    "# Construct statements from each correct_answer and incorrect_answer:\n",
    "correct_statements = []\n",
    "incorrect_statements = []\n",
    "for e in truthfulqa['validation']:\n",
    "    for correct_answer in e['correct_answers']:\n",
    "        correct_statements.append(f\"{e['question']} {correct_answer}.\")\n",
    "    for incorrect_answer in e['incorrect_answers']:\n",
    "        incorrect_statements.append(f\"{e['question']} {incorrect_answer}.\")\n",
    "pp(len(incorrect_statements))\n",
    "pp(len(correct_statements))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Found cached dataset truthful_qa (/root/.cache/huggingface/datasets/truthful_qa/generation/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 711.02it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3318\n",
      "2600\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Create dataset with x as concatenated correct and incorrect 2..4 statements,\n",
    "# and y as several 0 or 1 depending on whether a correct or incorrect statement is the correct answer.\n",
    "dataset = []    \n",
    "tokenizer = gpt2_xl.tokenizer\n",
    "while correct_statements or incorrect_statements:\n",
    "    x : torch.Tensor = None\n",
    "    y = []\n",
    "    for _ in range(np.random.randint(2, 5)):\n",
    "        label =  np.random.randint(2)\n",
    "        statements = (correct_statements, incorrect_statements)[label]\n",
    "        if statements:\n",
    "            tokens = tokenizer.encode( statements.pop(), return_tensors='pt')\n",
    "            x = tokens if x is None else torch.concat((x, tokens), -1)\n",
    "            inx = tokens.shape[1] + (y[-1][0] if y else 0)\n",
    "            y.append((inx, label))\n",
    "    if x is not None:\n",
    "        x.squeeze_(0)\n",
    "        dataset.append((x, y))\n",
    "pp(dataset[0])        "
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'GPT2Model' object has no attribute 'tokenizer'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/root/artem_capstone/dlk.py\u001b[0m in \u001b[0;36mline 5\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=107'>108</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=108'>109</a>\u001b[0m \u001b[39m# Create dataset with x as concatenated correct and incorrect 2..4 statements,\u001b[39;00m\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=109'>110</a>\u001b[0m \u001b[39m# and y as several 0 or 1 depending on whether a correct or incorrect statement is the correct answer.\u001b[39;00m\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=110'>111</a>\u001b[0m dataset \u001b[39m=\u001b[39m []    \n\u001b[0;32m----> <a href='file:///root/artem_capstone/dlk.py?line=111'>112</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m gpt2_xl\u001b[39m.\u001b[39;49mtokenizer\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=112'>113</a>\u001b[0m \u001b[39mwhile\u001b[39;00m correct_statements \u001b[39mor\u001b[39;00m incorrect_statements:\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=113'>114</a>\u001b[0m     x : torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1611'>1612</a>\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1612'>1613</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1613'>1614</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1614'>1615</a>\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2Model' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Create dataset with x as concatenated correct and incorrect 2..4 statements,\n",
    "# and y as several 0 or 1 depending on whether a correct or incorrect statement is the correct answer.\n",
    "dataset = []    \n",
    "#tokenizer = gpt2_xl.tokenizer\n",
    "while correct_statements or incorrect_statements:\n",
    "    x : torch.Tensor = None\n",
    "    y = []\n",
    "    for _ in range(np.random.randint(2, 5)):\n",
    "        label =  np.random.randint(2)\n",
    "        statements = (correct_statements, incorrect_statements)[label]\n",
    "        if statements:\n",
    "            tokens = tokenizer.encode( statements.pop(), return_tensors='pt')\n",
    "            x = tokens if x is None else torch.concat((x, tokens), -1)\n",
    "            inx = tokens.shape[1] + (y[-1][0] if y else 0)\n",
    "            y.append((inx, label))\n",
    "    if x is not None:\n",
    "        x.squeeze_(0)\n",
    "        dataset.append((x, y))\n",
    "pp(dataset[0])        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(tensor([16973,   262,  9329,   527,   456, 25201,  1683, 16019,    30,  1400,\n",
      "           11,   262,  9329,   527,   456, 25201,   318,   257,  5863,  4692,\n",
      "         1339,    13, 16973,   262,  9329,   527,   456, 25201,  1683, 16019,\n",
      "           30,  1400,    11,   262,  9329,   527,   456, 25201,   373,  1239,\n",
      "        16019,    13, 16973, 10584, 34405,  6776,   618,   673,  2627,   257,\n",
      "        32292,    30,  3363,    11, 10584, 34405,  2627,   257, 32292,   878,\n",
      "          607,  1918,    13, 16973, 10584, 34405,  6776,   618,   673,  2627,\n",
      "          257, 32292,    30,  3363,    11, 10584, 34405,   373,  6776,   618,\n",
      "          673,  2627,   257, 32292,    13]),\n",
      " [(22, 1), (42, 1), (63, 1), (85, 1)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# with torch.inference_mode():\n",
    "#     _, cache_true = gpt2_xl.run_with_cache(dataset[0][0])\n",
    "# pp(cache_true['mlp_out', 47].shape)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = gpt2_xl.forward(dataset[0][0], output_hidden_states=True)\n",
    "    cache_true = output['hidden_states']\n",
    "pp(cache_true)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(tensor([[[ 0.0805, -0.0513, -0.0210,  ...,  0.0625,  0.1481,  0.0381],\n",
      "         [-0.1280,  0.0088,  0.0293,  ...,  0.0033, -0.2571, -0.0215],\n",
      "         [-0.0957, -0.0380, -0.0340,  ...,  0.0040, -0.2119,  0.0191],\n",
      "         ...,\n",
      "         [ 0.0025,  0.0133,  0.0143,  ..., -0.0014, -0.0436, -0.0098],\n",
      "         [ 0.0574, -0.0342, -0.0722,  ...,  0.0568, -0.0885, -0.0028],\n",
      "         [ 0.0008, -0.0079,  0.0302,  ...,  0.0045, -0.0500,  0.0347]]]),\n",
      " tensor([[[ 0.2435, -0.0232,  0.0165,  ...,  0.8927, -0.7227,  0.0826],\n",
      "         [-0.5445, -0.5327,  0.2404,  ..., -0.5372, -0.1643, -0.7119],\n",
      "         [ 0.7539,  0.4710,  0.5205,  ..., -0.6841, -1.1856,  0.4466],\n",
      "         ...,\n",
      "         [-0.3641,  0.0144,  0.0387,  ...,  0.0152, -0.2416, -0.0685],\n",
      "         [-0.2612, -0.4375,  0.8180,  ..., -0.0188, -0.9010, -0.9666],\n",
      "         [ 0.1159,  0.1858,  0.1609,  ..., -0.3160,  0.0206, -0.2996]]]),\n",
      " tensor([[[ 0.0727,  0.2444,  0.1474,  ...,  0.6466, -0.9571, -0.5244],\n",
      "         [-0.7441, -0.7319,  0.1189,  ..., -1.1947,  0.2070, -0.8684],\n",
      "         [ 1.1823,  0.7385,  1.5367,  ..., -1.6446, -1.5685, -0.1691],\n",
      "         ...,\n",
      "         [-0.3446,  0.0408,  0.4804,  ...,  0.0234, -0.2047, -0.0635],\n",
      "         [-0.6118, -0.8464,  1.1573,  ...,  0.2120, -0.6851, -0.9444],\n",
      "         [-0.1687,  0.1446,  0.2861,  ..., -0.5718, -0.2480, -0.4006]]]),\n",
      " tensor([[[ 0.4651,  0.2305, -0.2809,  ...,  0.4675, -1.0606, -0.5300],\n",
      "         [-1.0309, -0.5995, -0.2319,  ..., -1.4540,  0.2080, -0.7238],\n",
      "         [ 2.0149,  1.6173,  1.9431,  ..., -1.4073, -2.1549, -0.1374],\n",
      "         ...,\n",
      "         [-0.2247,  0.1622,  0.5672,  ...,  0.3093, -0.3470, -0.0420],\n",
      "         [-1.1297, -0.3658,  1.7885,  ..., -0.1024, -0.7825, -1.1289],\n",
      "         [-0.1492,  0.2701,  0.3269,  ..., -0.6160, -0.2549, -0.3203]]]),\n",
      " tensor([[[ 0.5810,  0.4981, -0.1705,  ...,  0.2681, -0.4001, -0.1290],\n",
      "         [-1.1266, -0.4291, -0.4629,  ..., -1.8405,  0.2536, -0.3507],\n",
      "         [ 2.2452,  1.5273,  2.2599,  ..., -2.0916, -2.5760, -0.1065],\n",
      "         ...,\n",
      "         [-0.1835,  0.1398,  0.8457,  ...,  0.5771, -0.4072,  0.1840],\n",
      "         [-1.6256, -0.0704,  2.3471,  ..., -0.3092, -0.5950, -1.5537],\n",
      "         [-0.1059,  0.4573,  0.4608,  ..., -0.5543, -0.3792, -0.5696]]]),\n",
      " tensor([[[ 0.7597,  0.7568, -0.4258,  ..., -0.4503, -1.0166, -0.2115],\n",
      "         [-1.1047, -0.3591, -0.8864,  ..., -2.6164,  0.3088, -0.2154],\n",
      "         [ 2.6099,  2.0502,  1.6810,  ..., -2.2579, -3.0766,  0.2510],\n",
      "         ...,\n",
      "         [ 0.0898,  0.1427,  0.8998,  ...,  0.5473, -0.6406, -0.1597],\n",
      "         [-1.2917, -0.2004,  2.4132,  ..., -0.6075, -0.6187, -1.8289],\n",
      "         [ 0.1640,  0.4908,  0.5946,  ..., -0.4508, -0.5655, -0.7284]]]),\n",
      " tensor([[[ 1.1974,  0.5523, -0.1469,  ..., -1.3036, -0.6545, -0.4172],\n",
      "         [-1.5003, -0.2705, -0.7145,  ..., -2.8047,  0.4606,  0.0055],\n",
      "         [ 2.9437,  2.0508,  1.6573,  ..., -1.8272, -3.1159,  0.3628],\n",
      "         ...,\n",
      "         [ 0.0980,  0.2504,  0.9354,  ...,  0.5428, -0.6519, -0.2614],\n",
      "         [-1.3827, -0.4635,  2.6277,  ..., -1.0684, -1.2342, -1.9237],\n",
      "         [ 0.2189,  0.4256,  0.5565,  ..., -0.3445, -0.6053, -0.9874]]]),\n",
      " tensor([[[ 1.6279,  0.5216, -1.0351,  ..., -1.7499, -0.8893,  0.1322],\n",
      "         [-1.6535, -0.3500, -0.5114,  ..., -2.8476,  0.6560, -0.0664],\n",
      "         [ 2.2500,  1.9817,  1.8398,  ..., -2.0463, -3.4540,  0.4088],\n",
      "         ...,\n",
      "         [ 0.3716,  0.2548,  0.9879,  ...,  0.5363, -0.4364, -0.5807],\n",
      "         [-1.6194, -0.1379,  3.1271,  ..., -0.7400, -1.9307, -2.1103],\n",
      "         [ 0.3018,  0.2304,  0.5972,  ..., -0.4987, -0.6818, -0.9415]]]),\n",
      " tensor([[[ 0.9929,  0.8351, -0.9729,  ..., -2.4814, -0.5459, -0.7341],\n",
      "         [-1.9842, -0.3933, -0.5540,  ..., -3.1275,  0.9761,  0.4183],\n",
      "         [ 2.0509,  2.0891,  1.8300,  ..., -1.3356, -2.9587,  0.4840],\n",
      "         ...,\n",
      "         [ 0.5490,  0.0421,  1.4028,  ...,  0.4369, -1.0359, -0.3808],\n",
      "         [-1.5920,  0.3609,  3.2204,  ..., -1.3075, -2.3541, -2.5491],\n",
      "         [ 0.0190, -0.1832,  0.9757,  ..., -0.5349, -0.7551, -0.8407]]]),\n",
      " tensor([[[ 1.4018,  0.4095, -0.9982,  ..., -3.2170, -0.4839, -0.8665],\n",
      "         [-2.8336, -0.3215, -0.7194,  ..., -2.5012,  1.2407,  0.2710],\n",
      "         [ 1.6249,  2.2026,  1.4333,  ..., -0.8942, -2.6740,  0.8176],\n",
      "         ...,\n",
      "         [ 0.3109, -0.2696,  1.2530,  ...,  0.7447, -0.9918, -0.5636],\n",
      "         [-2.0945,  0.3168,  3.4833,  ..., -2.0592, -2.7259, -3.0880],\n",
      "         [ 0.2281, -0.0899,  0.8959,  ..., -0.6118, -0.8207, -0.7835]]]),\n",
      " tensor([[[ 1.5697,  0.1501, -1.1864,  ..., -3.9588, -0.5844, -0.8893],\n",
      "         [-2.7110, -0.4312, -0.4935,  ..., -2.9457,  1.5637,  0.8589],\n",
      "         [ 1.7458,  1.8623,  0.8770,  ..., -0.3630, -2.8230,  0.5485],\n",
      "         ...,\n",
      "         [ 0.5993, -0.7057,  1.7883,  ...,  0.8578, -0.6029, -0.5176],\n",
      "         [-1.5209,  0.2432,  3.6227,  ..., -2.4006, -2.8000, -2.4636],\n",
      "         [ 0.3245, -0.4718,  1.2794,  ..., -0.9731, -1.0960, -0.8726]]]),\n",
      " tensor([[[ 1.6016,  0.1188, -1.3532,  ..., -4.4564, -0.6185, -0.9511],\n",
      "         [-2.7211, -1.0588, -0.6920,  ..., -2.8967,  1.6817,  1.1315],\n",
      "         [ 1.4171,  1.5920,  0.5975,  ..., -0.1381, -2.6509,  0.7341],\n",
      "         ...,\n",
      "         [ 0.3813, -0.8718,  1.5569,  ...,  1.3382, -0.5829, -0.3939],\n",
      "         [-1.3102,  0.1796,  2.8793,  ..., -2.4471, -2.7839, -3.0167],\n",
      "         [ 0.0554, -0.9378,  1.2533,  ..., -0.0713, -0.5954, -0.9207]]]),\n",
      " tensor([[[ 1.7301,  0.0728, -1.3128,  ..., -4.9075, -0.7240, -1.1232],\n",
      "         [-2.4947, -1.1844, -0.5348,  ..., -2.6630,  1.4876,  0.6105],\n",
      "         [ 1.4157,  1.8704,  0.9629,  ...,  0.2624, -2.6332,  0.8444],\n",
      "         ...,\n",
      "         [ 0.3117, -0.7825,  1.4304,  ...,  1.3275, -0.3974, -0.4131],\n",
      "         [-1.7241,  0.1322,  3.4011,  ..., -2.6115, -3.0157, -3.0756],\n",
      "         [-0.6436, -0.8679,  1.0391,  ..., -0.1559, -0.7737, -1.2846]]]),\n",
      " tensor([[[ 1.8505,  0.0111, -1.2864,  ..., -5.4285, -0.5743, -1.1601],\n",
      "         [-2.4075, -0.9759, -0.6567,  ..., -3.1286,  1.7721,  0.7180],\n",
      "         [ 1.3043,  1.9645,  1.2264,  ...,  0.3374, -2.4293,  1.0781],\n",
      "         ...,\n",
      "         [ 0.8970, -0.7196,  1.4047,  ...,  1.3204, -0.4495, -0.0537],\n",
      "         [-1.4911, -0.4867,  3.5019,  ..., -3.1980, -2.7145, -2.9216],\n",
      "         [-0.5032, -0.7355,  0.7244,  ..., -0.6618, -1.2348, -1.6750]]]),\n",
      " tensor([[[ 2.0507, -0.0717, -1.3101,  ..., -5.9294, -0.6495, -1.2474],\n",
      "         [-2.7957, -1.5117, -0.5104,  ..., -3.7274,  1.9888,  0.3866],\n",
      "         [ 0.7710,  1.9261,  1.5225,  ...,  0.3510, -2.2951,  1.6002],\n",
      "         ...,\n",
      "         [ 1.2117, -0.5760,  1.6638,  ...,  0.7250, -0.3375, -0.2660],\n",
      "         [-1.7996, -0.5965,  3.4568,  ..., -3.2341, -2.8019, -3.2931],\n",
      "         [-0.4215, -0.6335,  0.6192,  ..., -0.4780, -0.8292, -1.5788]]]),\n",
      " tensor([[[ 2.2155, -0.0420, -1.4565,  ..., -6.4750, -0.7175, -1.3887],\n",
      "         [-2.5201, -1.8241, -0.6734,  ..., -4.1366,  2.5486,  0.2991],\n",
      "         [ 0.5499,  1.9442,  1.4016,  ...,  0.0119, -2.1278,  2.4500],\n",
      "         ...,\n",
      "         [ 1.1553, -0.7467,  1.6463,  ...,  0.1372, -0.4122, -0.1477],\n",
      "         [-1.0437, -0.4901,  3.4091,  ..., -3.7005, -2.8293, -2.6273],\n",
      "         [-0.7360, -0.4798,  0.6620,  ..., -0.8113, -0.8656, -1.2999]]]),\n",
      " tensor([[[ 2.2694, -0.0311, -1.5948,  ..., -7.0205, -0.5812, -1.5134],\n",
      "         [-2.2686, -1.5261, -0.7441,  ..., -3.3109,  3.2285,  0.3646],\n",
      "         [ 0.7142,  2.3212,  1.1424,  ...,  0.1408, -1.9386,  2.3713],\n",
      "         ...,\n",
      "         [ 1.3047, -1.4506,  1.2856,  ..., -0.3810, -0.4970,  0.3497],\n",
      "         [-0.4839, -0.4496,  2.9902,  ..., -3.8415, -3.2710, -2.6190],\n",
      "         [-1.2399, -0.3283,  0.4291,  ..., -1.5916, -1.0474, -1.4947]]]),\n",
      " tensor([[[ 2.3827, -0.0773, -1.5438,  ..., -7.7551, -0.4900, -1.6417],\n",
      "         [-2.7161, -1.2876, -0.9162,  ..., -3.9366,  3.1498,  0.9106],\n",
      "         [ 1.0940,  2.4381,  1.5588,  ..., -0.1286, -2.1397,  2.4460],\n",
      "         ...,\n",
      "         [ 1.9788, -0.9638,  1.3881,  ..., -0.4606, -0.7434,  0.2719],\n",
      "         [-1.1592,  0.2916,  2.8813,  ..., -4.6409, -3.5190, -2.4351],\n",
      "         [-1.4090,  0.2525,  0.6138,  ..., -1.1486, -1.5694, -1.3321]]]),\n",
      " tensor([[[ 2.5589, -0.0119, -1.4202,  ..., -8.5618, -0.5379, -1.6930],\n",
      "         [-2.3451, -1.3379, -0.0222,  ..., -3.8867,  2.3936,  1.1396],\n",
      "         [ 1.5420,  2.7741,  1.2850,  ..., -0.1774, -2.3857,  2.9026],\n",
      "         ...,\n",
      "         [ 1.9321, -0.7047,  1.2062,  ..., -1.1924, -0.7216,  0.5329],\n",
      "         [-1.2013,  0.9816,  2.2234,  ..., -5.4679, -4.0469, -2.3202],\n",
      "         [-1.8250,  0.8358,  0.4925,  ..., -0.8176, -1.0672, -1.4660]]]),\n",
      " tensor([[[ 2.7018e+00, -4.1532e-02, -1.5027e+00,  ..., -9.2501e+00,\n",
      "          -3.6784e-01, -1.7127e+00],\n",
      "         [-2.2571e+00, -2.1682e+00,  5.6736e-01,  ..., -3.3462e+00,\n",
      "           2.3333e+00, -6.1026e-02],\n",
      "         [ 1.1472e+00,  3.0988e+00,  1.1994e+00,  ..., -4.5644e-03,\n",
      "          -2.1928e+00,  3.0334e+00],\n",
      "         ...,\n",
      "         [ 2.7142e+00, -3.0658e-01,  1.5003e+00,  ..., -1.0566e+00,\n",
      "           2.8835e-01,  9.9016e-01],\n",
      "         [-4.4581e-01,  9.1795e-01,  1.1411e+00,  ..., -5.8722e+00,\n",
      "          -2.8958e+00, -1.6935e+00],\n",
      "         [-2.2476e+00,  1.0148e+00,  4.6499e-01,  ..., -1.6409e+00,\n",
      "          -1.2105e+00, -1.1526e+00]]]),\n",
      " tensor([[[ 2.7668, -0.1830, -1.4778,  ..., -9.9895, -0.2480, -1.8815],\n",
      "         [-2.8001, -2.1446, -0.3270,  ..., -3.0083,  2.7662, -1.1444],\n",
      "         [ 1.3108,  2.7914,  1.1683,  ...,  0.0887, -1.5581,  2.6884],\n",
      "         ...,\n",
      "         [ 3.4400,  0.4486,  2.2676,  ..., -1.7023,  1.5699,  0.7811],\n",
      "         [-0.3404,  1.2458,  0.1005,  ..., -6.7971, -1.7596, -2.0785],\n",
      "         [-3.3083,  2.2915, -0.7753,  ..., -1.9575,  0.5339, -2.2959]]]),\n",
      " tensor([[[  2.9017,  -0.2523,  -1.4069,  ..., -10.7977,  -0.1024,  -1.8967],\n",
      "         [ -2.8209,  -3.0396,  -0.5166,  ...,  -2.8025,   2.2092,  -0.7357],\n",
      "         [  1.2181,   2.9022,   1.3761,  ...,   0.4060,  -1.5589,   3.1651],\n",
      "         ...,\n",
      "         [  3.9269,   0.8638,   2.0399,  ...,  -1.9804,   2.1050,   1.3912],\n",
      "         [ -1.9192,   1.8588,  -0.5179,  ...,  -7.0897,  -2.1584,  -2.0870],\n",
      "         [ -4.1606,   2.0248,  -1.8285,  ...,  -2.8218,   1.2474,  -2.6297]]]),\n",
      " tensor([[[  2.9841,  -0.2508,  -1.2628,  ..., -11.6736,   0.0839,  -1.9355],\n",
      "         [ -2.6864,  -3.2018,  -0.8927,  ...,  -2.4818,   1.6217,  -0.5214],\n",
      "         [  0.4650,   3.0567,   1.6590,  ...,   0.6963,  -1.5516,   3.0642],\n",
      "         ...,\n",
      "         [  3.1267,   0.6607,   1.6428,  ...,  -1.4436,   1.0989,   1.1269],\n",
      "         [ -2.7340,   1.4055,  -1.1493,  ...,  -6.7946,  -2.8002,  -2.0546],\n",
      "         [ -4.8265,   2.7136,  -1.7142,  ...,  -3.2800,   2.1649,  -3.0314]]]),\n",
      " tensor([[[  2.9730,  -0.3807,  -1.1508,  ..., -12.4496,   0.2997,  -2.0541],\n",
      "         [ -3.9356,  -4.2491,  -0.3940,  ...,  -3.2265,   1.6383,  -0.5443],\n",
      "         [  0.1339,   2.5572,   1.0891,  ...,   0.7094,  -1.6839,   3.3390],\n",
      "         ...,\n",
      "         [  3.9194,   0.2165,   1.7552,  ...,  -1.2222,   2.0351,   0.3318],\n",
      "         [ -3.2209,  -0.0374,  -0.9203,  ...,  -8.2288,  -1.5145,  -3.2641],\n",
      "         [ -6.1456,   1.6211,  -1.2475,  ...,  -3.8437,   2.7075,  -2.9080]]]),\n",
      " tensor([[[  2.9787,  -0.4762,  -1.1076,  ..., -13.1591,   0.5294,  -2.1311],\n",
      "         [ -4.6345,  -3.4808,  -0.4231,  ...,  -3.8224,   1.8654,  -0.2692],\n",
      "         [  0.4423,   2.3050,   0.7675,  ...,   0.5520,  -1.9783,   2.8182],\n",
      "         ...,\n",
      "         [  4.0748,  -0.4475,   1.4057,  ...,  -1.4187,   3.4677,   0.6490],\n",
      "         [ -4.0414,  -0.3215,  -1.3676,  ...,  -9.0074,  -2.6004,  -3.7219],\n",
      "         [ -7.2351,   1.2833,  -1.3062,  ...,  -3.6371,   4.2192,  -2.2514]]]),\n",
      " tensor([[[  2.8833,  -0.6375,  -1.0340,  ..., -13.7922,   0.8049,  -2.0974],\n",
      "         [ -6.1461,  -3.4695,  -0.7525,  ...,  -3.9441,   1.7166,  -0.4365],\n",
      "         [ -0.5031,   2.4078,   0.9352,  ...,  -0.1854,  -1.5472,   2.6076],\n",
      "         ...,\n",
      "         [  4.3156,  -1.2474,   1.9238,  ...,  -0.9822,   4.1541,   1.7112],\n",
      "         [ -4.3015,  -0.5132,  -0.9608,  ...,  -8.7532,  -2.2187,  -3.0074],\n",
      "         [ -6.8166,   1.0125,  -1.7174,  ...,  -3.3577,   5.2042,  -2.2287]]]),\n",
      " tensor([[[  2.8648,  -0.5814,  -0.8991,  ..., -14.3576,   1.1358,  -2.2079],\n",
      "         [ -6.8370,  -3.4429,  -1.6425,  ...,  -4.2685,   1.9757,   0.0487],\n",
      "         [ -0.4163,   1.6447,   2.1322,  ...,  -0.2008,  -1.2632,   2.6498],\n",
      "         ...,\n",
      "         [  5.5097,  -3.2533,   1.7996,  ...,  -0.1594,   4.3938,   2.1366],\n",
      "         [ -3.4289,  -0.5839,  -0.5702,  ...,  -9.1191,  -2.8202,  -3.1377],\n",
      "         [ -6.3918,   1.8488,  -1.1943,  ...,  -3.5928,   6.6560,  -3.4809]]]),\n",
      " tensor([[[  2.8794,  -0.6225,  -0.8725,  ..., -14.8867,   1.4966,  -2.3120],\n",
      "         [ -6.9540,  -3.4198,  -1.3592,  ...,  -4.8046,   2.4431,   0.1056],\n",
      "         [ -1.0310,   0.6282,   2.2953,  ...,  -0.1687,  -0.9788,   3.1608],\n",
      "         ...,\n",
      "         [  6.1064,  -4.1739,   2.4136,  ...,   1.5344,   5.6778,   2.5141],\n",
      "         [ -3.7006,  -1.0691,  -1.7356,  ...,  -8.6394,  -2.6701,  -3.1759],\n",
      "         [ -6.1912,   1.5522,  -0.9990,  ...,  -4.6307,   6.9298,  -2.1856]]]),\n",
      " tensor([[[  2.8084,  -0.6981,  -0.7342,  ..., -15.8001,   1.7328,  -2.3023],\n",
      "         [ -6.8460,  -3.5325,  -1.2915,  ...,  -5.1039,   2.6713,   1.4023],\n",
      "         [ -0.7601,   0.6144,   2.2478,  ...,  -0.1445,  -1.5059,   3.6628],\n",
      "         ...,\n",
      "         [  5.2972,  -4.6158,   4.1445,  ...,   3.0052,   4.7848,   1.1666],\n",
      "         [ -4.9399,  -2.5363,  -2.0114,  ...,  -9.7735,  -2.5496,  -3.0633],\n",
      "         [ -5.6026,   2.2770,  -0.7674,  ...,  -5.3181,   8.3656,  -1.8875]]]),\n",
      " tensor([[[  2.8080,  -0.8074,  -0.6910,  ..., -16.8523,   1.9711,  -2.3088],\n",
      "         [ -7.0809,  -5.1237,  -0.7337,  ...,  -5.5967,   3.3051,   2.4799],\n",
      "         [ -0.1601,  -0.1882,   2.1844,  ...,  -0.6738,  -3.0239,   4.4453],\n",
      "         ...,\n",
      "         [  6.2315,  -4.1228,   3.0470,  ...,   4.0745,   7.3103,   1.0794],\n",
      "         [ -6.2329,  -2.9681,  -3.2392,  ..., -12.0494,  -1.3834,  -3.1039],\n",
      "         [ -6.1365,   3.5927,  -1.2816,  ...,  -7.5691,   7.6276,  -3.4808]]]),\n",
      " tensor([[[  2.7007,  -0.9370,  -0.5753,  ..., -17.9622,   2.2806,  -2.1972],\n",
      "         [ -7.8282,  -4.8155,  -1.2027,  ...,  -7.5948,   2.8186,   2.6238],\n",
      "         [ -0.1606,  -0.1889,   1.9896,  ...,  -1.4432,  -2.4952,   5.5614],\n",
      "         ...,\n",
      "         [  6.1068,  -4.1775,   2.9722,  ...,   4.4437,   9.1032,   0.1843],\n",
      "         [ -6.4448,  -2.1429,  -4.2475,  ..., -12.5010,  -0.7171,  -4.5954],\n",
      "         [ -5.4159,   5.6334,  -0.0750,  ...,  -7.7012,   7.6228,  -2.6649]]]),\n",
      " tensor([[[  2.5703,  -1.0075,  -0.5683,  ..., -19.4141,   2.5464,  -2.1700],\n",
      "         [ -7.5921,  -4.7454,  -1.6294,  ...,  -9.0935,   4.0222,   3.7535],\n",
      "         [  0.1987,  -0.7944,   1.8661,  ...,  -2.5557,  -2.2483,   5.3803],\n",
      "         ...,\n",
      "         [  6.3342,  -3.9415,   3.2637,  ...,   3.8966,   9.4931,   0.3116],\n",
      "         [ -6.8517,  -3.5796,  -5.6816,  ..., -13.5211,  -1.7451,  -6.0193],\n",
      "         [ -4.9264,   5.8755,   0.8481,  ...,  -7.5678,   8.2717,  -2.8750]]]),\n",
      " tensor([[[  2.4063,  -1.0089,  -0.4925,  ..., -20.8191,   2.8891,  -2.1367],\n",
      "         [ -8.6473,  -4.8037,  -1.8699,  ..., -11.4811,   4.2250,   4.0159],\n",
      "         [  0.1095,  -1.7887,   1.9315,  ...,  -2.5177,  -1.7287,   4.7023],\n",
      "         ...,\n",
      "         [  6.0787,  -5.3507,   3.4689,  ...,   2.7915,   8.8795,  -0.2539],\n",
      "         [ -8.0550,  -4.7406,  -6.0313,  ..., -16.5035,  -1.8986,  -7.5545],\n",
      "         [ -3.7911,   6.7832,   0.7189,  ...,  -8.5157,   9.3005,  -2.8153]]]),\n",
      " tensor([[[  2.4306,  -0.9780,  -0.2892,  ..., -22.4566,   2.9861,  -2.1140],\n",
      "         [ -8.9431,  -5.1005,  -1.5929,  ..., -12.4464,   5.3307,   4.7913],\n",
      "         [ -0.2958,  -2.1238,   1.5355,  ...,  -4.1666,  -2.6523,   4.8553],\n",
      "         ...,\n",
      "         [  6.4940,  -5.7068,   3.5540,  ...,   1.6910,   8.9118,  -0.0973],\n",
      "         [ -9.3202,  -5.4990,  -7.2560,  ..., -17.5933,  -2.1779,  -7.0251],\n",
      "         [ -5.0428,   7.8028,   0.6687,  ..., -10.1471,  10.2615,  -0.9677]]]),\n",
      " tensor([[[  2.4630,  -0.9012,  -0.1638,  ..., -24.2311,   3.2344,  -2.0178],\n",
      "         [ -9.4144,  -6.2223,  -1.0831,  ..., -13.3748,   6.4444,   5.2297],\n",
      "         [ -1.4189,  -3.7424,   1.8487,  ...,  -5.7048,  -2.7734,   5.2783],\n",
      "         ...,\n",
      "         [  5.0332,  -6.2080,   3.5900,  ...,   0.6809,   8.8112,   0.0618],\n",
      "         [-10.9073,  -5.7135,  -9.7097,  ..., -19.1584,  -0.6041,  -7.9268],\n",
      "         [ -3.6565,   9.7274,   0.5684,  ..., -11.8642,  11.7723,  -1.2426]]]),\n",
      " tensor([[[  2.3681,  -0.8865,   0.0827,  ..., -26.3331,   3.3975,  -1.9752],\n",
      "         [ -9.8375,  -6.1540,  -1.2719,  ..., -15.8091,   7.5573,   5.8135],\n",
      "         [ -1.6336,  -3.0755,   1.9738,  ...,  -6.8173,  -3.4293,   5.8470],\n",
      "         ...,\n",
      "         [  5.1715,  -6.2682,   4.4468,  ...,  -1.1364,   9.6788,  -1.1396],\n",
      "         [-11.6153,  -6.8284,  -8.4626,  ..., -20.4316,  -1.1811,  -8.8057],\n",
      "         [ -2.4784,  11.6823,   2.4778,  ..., -13.1792,  13.9877,  -1.3496]]]),\n",
      " tensor([[[  2.2650,  -1.0233,   0.2030,  ..., -28.8871,   3.5412,  -1.9045],\n",
      "         [ -9.5527,  -7.4172,  -0.4086,  ..., -18.2389,   8.5939,   5.4126],\n",
      "         [ -2.0964,  -3.6817,   2.3271,  ...,  -7.3154,  -2.4396,   5.3886],\n",
      "         ...,\n",
      "         [  4.3463,  -5.3637,   5.4394,  ...,  -1.8755,  10.2430,  -2.6325],\n",
      "         [-13.6629,  -7.0337,  -8.5387,  ..., -23.1986,  -1.5132, -10.0768],\n",
      "         [ -1.8573,  11.4800,   4.2438,  ..., -14.7995,  15.7866,  -0.5044]]]),\n",
      " tensor([[[  2.2677,  -0.9088,   0.3111,  ..., -32.1163,   3.5613,  -1.8797],\n",
      "         [-10.0703,  -7.6282,  -0.5474,  ..., -21.4216,   9.2841,   5.8684],\n",
      "         [ -1.8178,  -4.5859,   2.4760,  ...,  -8.0145,  -1.9666,   6.5901],\n",
      "         ...,\n",
      "         [  2.8514,  -4.4316,   6.0106,  ...,  -2.1438,  10.4321,  -2.2044],\n",
      "         [-16.7344,  -8.0883,  -8.3072,  ..., -23.3134,  -2.5836, -11.2536],\n",
      "         [ -1.7821,  12.3021,   6.8286,  ..., -16.9864,  17.1302,   0.1486]]]),\n",
      " tensor([[[  2.2461,  -0.8183,   0.5166,  ..., -35.3225,   3.6628,  -1.7461],\n",
      "         [-10.1640,  -7.6558,  -1.2975,  ..., -23.6526,   9.4288,   4.5985],\n",
      "         [ -2.4084,  -3.7111,   2.5431,  ...,  -7.3721,  -2.0716,   7.3322],\n",
      "         ...,\n",
      "         [  2.7013,  -3.7583,   7.3880,  ...,  -3.1962,  11.0911,  -2.4923],\n",
      "         [-19.1428,  -8.4534,  -9.2590,  ..., -25.0145,  -4.5685, -12.7027],\n",
      "         [ -1.2880,  13.9798,   7.6665,  ..., -16.6338,  18.9254,   2.0863]]]),\n",
      " tensor([[[  2.2658,  -0.7703,   0.6428,  ..., -39.8152,   3.6570,  -1.6336],\n",
      "         [-11.0814,  -7.9109,  -2.2545,  ..., -27.4812,   9.4911,   4.4883],\n",
      "         [ -3.4717,  -3.7352,   2.5040,  ...,  -8.5453,  -1.5322,   8.4291],\n",
      "         ...,\n",
      "         [  0.9992,  -3.1379,   8.1298,  ...,  -1.7088,  12.7804,  -3.9495],\n",
      "         [-23.8259, -10.5417, -11.2527,  ..., -28.2481,  -9.3635, -14.9120],\n",
      "         [ -1.1063,  15.3840,   8.9835,  ..., -16.9314,  20.1959,   3.4614]]]),\n",
      " tensor([[[  2.0741,  -0.5813,   0.8345,  ..., -45.8011,   3.6839,  -1.5481],\n",
      "         [-11.2633,  -7.6542,  -2.2191,  ..., -32.7734,   9.9127,   4.3612],\n",
      "         [ -3.6518,  -4.1733,   2.9766,  ...,  -8.9046,  -1.4187,   9.0628],\n",
      "         ...,\n",
      "         [ -0.1711,  -1.1223,   8.7873,  ...,   0.3868,  14.0461,  -2.0947],\n",
      "         [-24.7576, -12.8097, -14.9443,  ..., -33.2687, -12.1063, -14.4668],\n",
      "         [  1.9026,  16.9265,   8.3454,  ..., -18.8606,  22.0119,   5.4845]]]),\n",
      " tensor([[[ 1.9123e+00, -6.2055e-01,  1.0976e+00,  ..., -5.5436e+01,\n",
      "           4.0624e+00, -1.4679e+00],\n",
      "         [-1.1672e+01, -7.5803e+00, -3.0633e+00,  ..., -3.6932e+01,\n",
      "           9.9616e+00,  4.5474e+00],\n",
      "         [-1.8522e+00, -3.8114e+00,  1.9249e+00,  ..., -1.0449e+01,\n",
      "           3.0710e-01,  7.1501e+00],\n",
      "         ...,\n",
      "         [-3.9549e+00,  1.0201e+00,  9.3658e+00,  ...,  4.6967e-02,\n",
      "           1.5981e+01, -2.0525e+00],\n",
      "         [-2.7467e+01, -1.6469e+01, -1.8869e+01,  ..., -3.7713e+01,\n",
      "          -1.6329e+01, -1.6912e+01],\n",
      "         [ 1.4520e+00,  1.6914e+01,  8.5741e+00,  ..., -2.3575e+01,\n",
      "           2.2693e+01,  7.4083e+00]]]),\n",
      " tensor([[[  1.7322,  -0.6016,   1.3402,  ..., -69.5875,   4.3621,  -1.4943],\n",
      "         [-11.4901,  -7.8940,  -3.7025,  ..., -43.4494,  11.4269,   4.2836],\n",
      "         [ -1.6771,  -3.7264,   1.8660,  ..., -11.7204,   2.3033,   7.4034],\n",
      "         ...,\n",
      "         [ -3.6304,   0.4912,   8.1118,  ...,  -3.2631,  16.6178,  -2.0053],\n",
      "         [-30.2582, -18.7712, -25.2471,  ..., -43.8160, -19.4658, -18.8064],\n",
      "         [  2.2294,  16.8167,   9.5128,  ..., -27.3403,  23.7078,   8.0111]]]),\n",
      " tensor([[[  1.6393,  -0.5495,   1.6005,  ..., -88.2134,   4.6309,  -1.4109],\n",
      "         [-11.5170,  -7.9618,  -4.9704,  ..., -49.2068,  11.9787,   3.9435],\n",
      "         [ -1.4449,  -4.9806,   1.1734,  ..., -13.3117,   1.5061,   8.0326],\n",
      "         ...,\n",
      "         [ -5.8518,   0.2117,   9.4831,  ...,  -7.7693,  18.1486,  -2.5936],\n",
      "         [-32.1747, -22.4648, -29.4381,  ..., -56.3326, -23.1599, -23.4176],\n",
      "         [  2.6261,  19.0354,  10.7832,  ..., -36.9337,  23.0690,  10.2945]]]),\n",
      " tensor([[[   1.3112,   -0.4720,    1.9456,  ..., -110.7149,    4.7507,\n",
      "            -1.3631],\n",
      "         [ -11.5325,   -8.4304,   -5.1660,  ...,  -65.7154,   12.2736,\n",
      "             4.2179],\n",
      "         [  -1.4231,   -5.1704,    0.8824,  ...,  -19.7005,    2.3551,\n",
      "             8.6117],\n",
      "         ...,\n",
      "         [  -7.8493,    0.2761,    8.7201,  ...,  -11.0547,   16.6228,\n",
      "            -2.4125],\n",
      "         [ -34.9822,  -26.8192,  -32.9021,  ...,  -82.3078,  -26.6333,\n",
      "           -26.7934],\n",
      "         [   3.6052,   20.9432,   11.6357,  ...,  -57.8645,   23.3772,\n",
      "            10.9483]]]),\n",
      " tensor([[[   1.2619,   -0.1716,    2.2755,  ..., -136.9916,    5.0800,\n",
      "            -1.1937],\n",
      "         [ -11.6168,   -8.7066,   -5.5527,  ...,  -92.5758,   12.5784,\n",
      "             5.1720],\n",
      "         [  -0.8836,   -4.6048,    2.8611,  ...,  -33.8859,    0.2365,\n",
      "             6.6042],\n",
      "         ...,\n",
      "         [  -8.9205,    1.1306,   10.0480,  ...,  -27.6801,   16.4848,\n",
      "            -2.9557],\n",
      "         [ -37.4006,  -28.5453,  -34.9625,  ..., -137.7280,  -29.8921,\n",
      "           -29.1121],\n",
      "         [   3.0751,   24.2644,   12.7163,  ...,  -96.3202,   26.1651,\n",
      "            11.2764]]]),\n",
      " tensor([[[ 1.1839e+00,  1.5074e-01,  2.5273e+00,  ..., -1.6676e+02,\n",
      "           5.0768e+00, -9.2612e-01],\n",
      "         [-1.1956e+01, -8.2102e+00, -6.2405e+00,  ..., -1.3790e+02,\n",
      "           1.2575e+01,  5.6209e+00],\n",
      "         [-1.1950e+00, -4.9645e+00,  4.3083e+00,  ..., -5.2557e+01,\n",
      "           2.4822e+00,  7.5087e+00],\n",
      "         ...,\n",
      "         [-1.0181e+01,  1.2675e-01,  1.1683e+01,  ..., -7.5491e+01,\n",
      "           1.7255e+01, -3.4102e+00],\n",
      "         [-4.1536e+01, -3.1467e+01, -3.6883e+01,  ..., -2.3439e+02,\n",
      "          -3.3171e+01, -3.2718e+01],\n",
      "         [ 4.5466e+00,  2.6075e+01,  1.4827e+01,  ..., -1.6785e+02,\n",
      "           2.6700e+01,  1.1815e+01]]]),\n",
      " tensor([[[   0.6606,    0.7907,    2.8914,  ..., -175.1510,    4.1128,\n",
      "            -0.5647],\n",
      "         [ -11.9189,   -7.8192,   -6.3960,  ..., -175.3327,   12.2093,\n",
      "             6.8924],\n",
      "         [  -1.7497,   -4.5074,    4.1052,  ...,  -80.5342,    3.1024,\n",
      "             7.5796],\n",
      "         ...,\n",
      "         [ -11.0010,    1.9392,   12.1936,  ..., -143.1310,   17.4011,\n",
      "            -1.1065],\n",
      "         [ -43.0479,  -33.2133,  -39.0496,  ..., -325.4391,  -34.4046,\n",
      "           -34.8339],\n",
      "         [   4.4728,   27.1698,   14.7928,  ..., -235.7465,   26.2198,\n",
      "            11.0902]]]),\n",
      " tensor([[-0.2415,  0.1255,  0.7011,  ..., -4.3075,  0.4613,  0.4116],\n",
      "        [-0.6465, -0.2066, -0.1157,  ..., -1.2411,  0.9307,  0.6750],\n",
      "        [-0.2888, -0.5198,  0.5735,  ..., -1.1081,  0.4030,  1.3063],\n",
      "        ...,\n",
      "        [-1.2344, -0.2475,  0.7674,  ..., -1.6353,  0.9360, -0.5863],\n",
      "        [-0.1142,  0.5915,  0.2337,  ..., -1.9109,  0.4153,  0.3588],\n",
      "        [-0.5158,  1.2398,  0.2653,  ..., -1.7796,  1.0053, -0.0351]]))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "reporter = elk.training.Reporter.load(f'./data/gpt2-xl/dbpedia_14/reporters/layer_47.pt', map_location=device)\n",
    "reporter.eval()\n",
    "pp(reporter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CcsReporter(\n",
      "  (norm): ConceptEraser()\n",
      "  (probe): Sequential(\n",
      "    (0): Linear(in_features=1600, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "pp(f'{len(cache)=}')\n",
    "pp(f'{cache[47]=}')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cache' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/artem_capstone/dlk.py\u001b[0m in \u001b[0;36mline 2\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=135'>136</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[0;32m----> <a href='file:///root/artem_capstone/dlk.py?line=136'>137</a>\u001b[0m pp(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(cache)\u001b[39m=}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=137'>138</a>\u001b[0m pp(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcache[\u001b[39m47\u001b[39m]\u001b[39m=}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cache' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# with torch.inference_mode():\n",
    "#     _, cache_true = gpt2_xl.run_with_cache(dataset[0][0])\n",
    "# pp(cache_true['mlp_out', 47].shape)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = gpt2_xl.forward(dataset[0][0], output_hidden_states=True)\n",
    "    cache = output['hidden_states']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "pp(f'{len(cache)=}')\n",
    "pp(f'{cache[47]=}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'len(cache)=49'\n",
      "('cache[47]=tensor([[[   0.6606,    0.7907,    2.8914,  ..., -175.1510,    '\n",
      " '4.1128,\\n'\n",
      " '            -0.5647],\\n'\n",
      " '         [ -11.9189,   -7.8192,   -6.3960,  ..., -175.3327,   12.2093,\\n'\n",
      " '             6.8924],\\n'\n",
      " '         [  -1.7497,   -4.5074,    4.1052,  ...,  -80.5342,    3.1024,\\n'\n",
      " '             7.5796],\\n'\n",
      " '         ...,\\n'\n",
      " '         [ -11.0010,    1.9392,   12.1936,  ..., -143.1310,   17.4011,\\n'\n",
      " '            -1.1065],\\n'\n",
      " '         [ -43.0479,  -33.2133,  -39.0496,  ..., -325.4391,  -34.4046,\\n'\n",
      " '           -34.8339],\\n'\n",
      " '         [   4.4728,   27.1698,   14.7928,  ..., -235.7465,   26.2198,\\n'\n",
      " '            11.0902]]])')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "pp(f'{len(cache)=}')\n",
    "pp(f'{cache[48].shape=}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'len(cache)=49'\n",
      "'cache[48].shape=torch.Size([85, 1600])'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "reporter = elk.training.Reporter.load(f'./data/gpt2-xl/dbpedia_14/reporters/layer_47.pt', map_location=device)\n",
    "reporter.eval()\n",
    "pp(reporter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CcsReporter(\n",
      "  (norm): ConceptEraser()\n",
      "  (probe): Sequential(\n",
      "    (0): Linear(in_features=1600, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[48][-1]).sigmoid()\n",
    "pp(res)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/root/artem_capstone/dlk.py\u001b[0m in \u001b[0;36mline 4\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=142'>143</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=143'>144</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=144'>145</a>\u001b[0m     \u001b[39m#res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\u001b[39;00m\n\u001b[0;32m----> <a href='file:///root/artem_capstone/dlk.py?line=145'>146</a>\u001b[0m     res \u001b[39m=\u001b[39m reporter(cache[\u001b[39m48\u001b[39;49m][\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39msigmoid()\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=146'>147</a>\u001b[0m pp(res)\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=147'>148</a>\u001b[0m pp(dataset[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1495'>1496</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1496'>1497</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1497'>1498</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1498'>1499</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1499'>1500</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1500'>1501</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1501'>1502</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1502'>1503</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/elk/elk/training/ccs_reporter.py:234\u001b[0m, in \u001b[0;36mCcsReporter.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='file:///root/elk/elk/training/ccs_reporter.py?line=231'>232</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    <a href='file:///root/elk/elk/training/ccs_reporter.py?line=232'>233</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the credence assigned to the hidden state `x`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///root/elk/elk/training/ccs_reporter.py?line=233'>234</a>\u001b[0m     raw_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(x))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///root/elk/elk/training/ccs_reporter.py?line=234'>235</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m raw_scores\u001b[39m.\u001b[39mmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale)\u001b[39m.\u001b[39madd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1495'>1496</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1496'>1497</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1497'>1498</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1498'>1499</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1499'>1500</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1500'>1501</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1501'>1502</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/.local/share/virtualenvs/artem_capstone-h3n8YrzE/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1502'>1503</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/elk/elk/training/concept_eraser.py:54\u001b[0m, in \u001b[0;36mConceptEraser.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///root/elk/elk/training/concept_eraser.py?line=50'>51</a>\u001b[0m \u001b[39massert\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m d\n\u001b[1;32m     <a href='file:///root/elk/elk/training/concept_eraser.py?line=52'>53</a>\u001b[0m \u001b[39m# First center the input\u001b[39;00m\n\u001b[0;32m---> <a href='file:///root/elk/elk/training/concept_eraser.py?line=53'>54</a>\u001b[0m x_ \u001b[39m=\u001b[39m x \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean_x\n\u001b[1;32m     <a href='file:///root/elk/elk/training/concept_eraser.py?line=55'>56</a>\u001b[0m \u001b[39m# Remove the subspace. We treat x_ as a batch of (1 x d) vectors\u001b[39;00m\n\u001b[1;32m     <a href='file:///root/elk/elk/training/concept_eraser.py?line=56'>57</a>\u001b[0m proj \u001b[39m=\u001b[39m (x_[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mNone\u001b[39;00m, :] \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu) \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu\u001b[39m.\u001b[39mmT\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# with torch.inference_mode():\n",
    "#     _, cache_true = gpt2_xl.run_with_cache(dataset[0][0])\n",
    "# pp(cache_true['mlp_out', 47].shape)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = gpt2_xl.forward(dataset[0][0], output_hidden_states=True)\n",
    "    cache = output['hidden_states'].to(device)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/root/artem_capstone/dlk.py\u001b[0m in \u001b[0;36mline 8\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=132'>133</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=133'>134</a>\u001b[0m     output \u001b[39m=\u001b[39m gpt2_xl\u001b[39m.\u001b[39mforward(dataset[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='file:///root/artem_capstone/dlk.py?line=134'>135</a>\u001b[0m     cache \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39;49m\u001b[39mhidden_states\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[48].to(device)[-1]).sigmoid()\n",
    "pp(res)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.3631, device='cuda:0')\n",
      "[(22, 1), (42, 1), (63, 1), (85, 1)]\n",
      "22 1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 21 is out of bounds for dimension 0 with size 0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/artem_capstone/dlk.py\u001b[0m in \u001b[0;36mline 9\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=148'>149</a>\u001b[0m \u001b[39mfor\u001b[39;00m inx, label \u001b[39min\u001b[39;00m dataset[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]:\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=149'>150</a>\u001b[0m     \u001b[39mprint\u001b[39m(inx, label)\n\u001b[0;32m----> <a href='file:///root/artem_capstone/dlk.py?line=150'>151</a>\u001b[0m     pp(res[inx\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 21 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "pp(f'{len(cache)=}')\n",
    "pp(f'{cache[48].shape=}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'len(cache)=49'\n",
      "'cache[48].shape=torch.Size([85, 1600])'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[48].to(device)).sigmoid()\n",
    "pp(res)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.5073, 0.5277, 0.4920, 0.4879, 0.4992, 0.5295, 0.5069, 0.4075, 0.4524,\n",
      "        0.4878, 0.4294, 0.5153, 0.4835, 0.4480, 0.3814, 0.4024, 0.5890, 0.4783,\n",
      "        0.5086, 0.4345, 0.4571, 0.4291, 0.4734, 0.5777, 0.4932, 0.4426, 0.4322,\n",
      "        0.4833, 0.4841, 0.4286, 0.4520, 0.4902, 0.4626, 0.4855, 0.4911, 0.4487,\n",
      "        0.4338, 0.4346, 0.5362, 0.4935, 0.4344, 0.3824, 0.5053, 0.4290, 0.4144,\n",
      "        0.4524, 0.5476, 0.4920, 0.4939, 0.4115, 0.4343, 0.4530, 0.4644, 0.4997,\n",
      "        0.4401, 0.4185, 0.4767, 0.4283, 0.5502, 0.4634, 0.3909, 0.4002, 0.3929,\n",
      "        0.4459, 0.3868, 0.4012, 0.4672, 0.5112, 0.4316, 0.4788, 0.4659, 0.4184,\n",
      "        0.4165, 0.4835, 0.4615, 0.4314, 0.4360, 0.5613, 0.4773, 0.5183, 0.4896,\n",
      "        0.4785, 0.4516, 0.4964, 0.3631], device='cuda:0')\n",
      "[(22, 1), (42, 1), (63, 1), (85, 1)]\n",
      "22 1\n",
      "tensor(0.4291, device='cuda:0')\n",
      "42 1\n",
      "tensor(0.3824, device='cuda:0')\n",
      "63 1\n",
      "tensor(0.3929, device='cuda:0')\n",
      "85 1\n",
      "tensor(0.3631, device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device)).sigmoid()\n",
    "pp(res)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.3050, 0.9935, 0.9124, 0.7929, 0.9053, 0.9734, 0.8366, 0.1308, 0.8611,\n",
      "         0.9044, 0.3054, 0.9358, 0.9229, 0.8388, 0.1966, 0.2088, 0.9983, 0.8936,\n",
      "         0.9640, 0.5051, 0.8237, 0.5409, 0.9603, 0.9991, 0.9582, 0.7825, 0.6045,\n",
      "         0.9315, 0.7711, 0.4138, 0.9390, 0.9523, 0.8545, 0.9958, 0.9755, 0.8188,\n",
      "         0.8179, 0.7204, 0.9831, 0.9190, 0.5723, 0.2966, 0.9685, 0.4541, 0.2366,\n",
      "         0.6938, 0.9950, 0.9063, 0.8278, 0.3497, 0.5442, 0.9244, 0.8659, 0.9859,\n",
      "         0.8131, 0.3358, 0.8373, 0.4717, 0.9989, 0.8913, 0.2677, 0.2569, 0.3490,\n",
      "         0.8148, 0.4340, 0.2194, 0.8731, 0.9939, 0.5128, 0.8555, 0.7232, 0.4452,\n",
      "         0.7410, 0.9695, 0.9449, 0.8220, 0.6168, 0.9952, 0.9450, 0.9938, 0.9010,\n",
      "         0.9101, 0.6735, 0.9850, 0.1229]], device='cuda:0')\n",
      "[(22, 1), (42, 1), (63, 1), (85, 1)]\n",
      "22 1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 21 is out of bounds for dimension 0 with size 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/artem_capstone/dlk.py\u001b[0m in \u001b[0;36mline 9\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=148'>149</a>\u001b[0m \u001b[39mfor\u001b[39;00m inx, label \u001b[39min\u001b[39;00m dataset[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]:\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=149'>150</a>\u001b[0m     \u001b[39mprint\u001b[39m(inx, label)\n\u001b[0;32m----> <a href='file:///root/artem_capstone/dlk.py?line=150'>151</a>\u001b[0m     pp(res[inx\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 21 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device)).sigmoid()\n",
    "pp(res.shape)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 85])\n",
      "[(22, 1), (42, 1), (63, 1), (85, 1)]\n",
      "22 1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 21 is out of bounds for dimension 0 with size 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/artem_capstone/dlk.py\u001b[0m in \u001b[0;36mline 9\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=148'>149</a>\u001b[0m \u001b[39mfor\u001b[39;00m inx, label \u001b[39min\u001b[39;00m dataset[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]:\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=149'>150</a>\u001b[0m     \u001b[39mprint\u001b[39m(inx, label)\n\u001b[0;32m----> <a href='file:///root/artem_capstone/dlk.py?line=150'>151</a>\u001b[0m     pp(res[inx\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 21 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device))[0].sigmoid()\n",
    "pp(res.shape)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([85])\n",
      "[(22, 1), (42, 1), (63, 1), (85, 1)]\n",
      "22 1\n",
      "tensor(0.5409, device='cuda:0')\n",
      "42 1\n",
      "tensor(0.2966, device='cuda:0')\n",
      "63 1\n",
      "tensor(0.3490, device='cuda:0')\n",
      "85 1\n",
      "tensor(0.1229, device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "reporter = elk.training.Reporter.load(f'./data/gpt2-xl/ag_news/reporters/layer_47.pt', map_location=device)\n",
    "reporter.eval()\n",
    "pp(reporter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CcsReporter(\n",
      "  (norm): ConceptEraser()\n",
      "  (probe): Sequential(\n",
      "    (0): Linear(in_features=1600, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device))[0].sigmoid()\n",
    "pp(res.shape)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([85])\n",
      "[(22, 1), (42, 1), (63, 1), (85, 1)]\n",
      "22 1\n",
      "tensor(0.7564, device='cuda:0')\n",
      "42 1\n",
      "tensor(0.4013, device='cuda:0')\n",
      "63 1\n",
      "tensor(0.5280, device='cuda:0')\n",
      "85 1\n",
      "tensor(0.4383, device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "reporter = elk.training.Reporter.load(f'./data/gpt2-xl/dbpedia_14/lr_models/layer_47.pt', map_location=device)\n",
    "reporter.eval()\n",
    "pp(reporter)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Expected a `dict` or `Reporter` object, but got <class 'list'>.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/artem_capstone/dlk.py\u001b[0m in \u001b[0;36mline 2\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=167'>168</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[0;32m----> <a href='file:///root/artem_capstone/dlk.py?line=168'>169</a>\u001b[0m reporter \u001b[39m=\u001b[39m elk\u001b[39m.\u001b[39;49mtraining\u001b[39m.\u001b[39;49mReporter\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data/gpt2-xl/dbpedia_14/lr_models/layer_47.pt\u001b[39;49m\u001b[39m'\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=169'>170</a>\u001b[0m reporter\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=170'>171</a>\u001b[0m pp(reporter)\n",
      "File \u001b[0;32m~/elk/elk/training/reporter.py:70\u001b[0m, in \u001b[0;36mReporter.load\u001b[0;34m(cls, path, map_location)\u001b[0m\n\u001b[1;32m     <a href='file:///root/elk/elk/training/reporter.py?line=67'>68</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m reporter\n\u001b[1;32m     <a href='file:///root/elk/elk/training/reporter.py?line=68'>69</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///root/elk/elk/training/reporter.py?line=69'>70</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     <a href='file:///root/elk/elk/training/reporter.py?line=70'>71</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a `dict` or `Reporter` object, but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obj)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///root/elk/elk/training/reporter.py?line=71'>72</a>\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a `dict` or `Reporter` object, but got <class 'list'>."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "tensor = torch.load(f'./data/gpt2-xl/dbpedia_14/lr_models/layer_47.pt', map_location=device)\n",
    "pp(tensor)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Classifier(\n",
      "  (linear): Linear(in_features=1600, out_features=1, bias=True)\n",
      ")]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "tensor = torch.load(f'./data/gpt2-xl/dbpedia_14/lr_models/layer_47.pt', map_location=device)\n",
    "pp(tensor)\n",
    "#reporter = elk.training.Reporter.load(f'./data/gpt2-xl/dbpedia_14/lr_models/layer_47.pt', map_location=device)\n",
    "#reporter.eval()\n",
    "#pp(reporter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Classifier(\n",
      "  (linear): Linear(in_features=1600, out_features=1, bias=True)\n",
      ")]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "reporter = torch.load(f'./data/gpt2-xl/dbpedia_14/lr_models/layer_47.pt', map_location=device)\n",
    "pp(reporter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Classifier(\n",
      "  (linear): Linear(in_features=1600, out_features=1, bias=True)\n",
      ")]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device))[0].sigmoid()\n",
    "pp(res.shape)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/artem_capstone/dlk.py\u001b[0m in \u001b[0;36mline 4\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=169'>170</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=170'>171</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=171'>172</a>\u001b[0m     \u001b[39m#res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\u001b[39;00m\n\u001b[0;32m----> <a href='file:///root/artem_capstone/dlk.py?line=172'>173</a>\u001b[0m     res \u001b[39m=\u001b[39m reporter(cache[\u001b[39m47\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device))[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msigmoid()\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=173'>174</a>\u001b[0m pp(res\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=174'>175</a>\u001b[0m pp(dataset[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device))\n",
    "    pp(res)\n",
    "    res = [0].sigmoid()\n",
    "pp(res.shape)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/artem_capstone/dlk.py\u001b[0m in \u001b[0;36mline 4\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=169'>170</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=170'>171</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=171'>172</a>\u001b[0m     \u001b[39m#res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\u001b[39;00m\n\u001b[0;32m----> <a href='file:///root/artem_capstone/dlk.py?line=172'>173</a>\u001b[0m     res \u001b[39m=\u001b[39m reporter(cache[\u001b[39m47\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=173'>174</a>\u001b[0m     pp(res)\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=174'>175</a>\u001b[0m     res \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msigmoid()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "reporter = torch.load(f'./data/gpt2-xl/dbpedia_14/lr_models/layer_47.pt', map_location=device)[0]\n",
    "pp(reporter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier(\n",
      "  (linear): Linear(in_features=1600, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device))\n",
    "    pp(res)\n",
    "    res = [0].sigmoid()\n",
    "pp(res.shape)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-3.1554e+04,  2.9691e+04,  4.8190e+04, -7.0022e+03,  3.0838e+04,\n",
      "          7.5812e+04, -1.6684e+04,  5.7554e+04,  5.1635e+04, -6.7735e+04,\n",
      "         -4.5031e+04,  2.9842e+04,  3.6086e+04,  1.5849e+04,  1.6806e+04,\n",
      "         -4.8002e+04,  4.2133e+04,  6.3501e+01,  1.2088e+05,  8.0037e+03,\n",
      "          5.9227e+04,  6.6244e+04,  3.0824e+04,  1.1024e+05,  2.6843e+04,\n",
      "         -2.0253e+03,  6.2194e+04,  6.2850e+03, -4.8561e+04,  3.5525e+04,\n",
      "          3.2681e+04, -7.6061e+04,  5.2919e+04,  9.0775e+04,  2.0617e+04,\n",
      "          2.8052e+03,  6.4431e+04,  1.5134e+04,  4.5953e+04, -1.9247e+04,\n",
      "          9.7630e+04,  2.7276e+04,  3.0647e+04,  4.2342e+04, -1.1101e+03,\n",
      "         -2.3795e+04,  1.6483e+05,  4.5880e+04,  6.5606e+04, -2.5157e+04,\n",
      "         -7.7115e+03,  9.4512e+04, -5.0809e+04,  7.8759e+04,  6.9595e+04,\n",
      "          2.9231e+03,  4.7988e+04,  3.6668e+03,  5.4307e+04,  3.1461e+04,\n",
      "         -4.5519e+03,  2.5983e+04,  4.1848e+04, -9.1882e+03,  1.9365e+04,\n",
      "         -5.7895e+03,  7.9265e+03,  4.5831e+04,  3.4234e+04, -4.4444e+03,\n",
      "          3.6480e+04, -5.6625e+04,  4.5559e+04, -5.6081e+04,  4.0064e+04,\n",
      "          3.1871e+04,  2.0469e+04,  3.7624e+04,  1.3394e+04,  1.0201e+05,\n",
      "          5.1215e+04,  1.5066e+04,  8.1638e+03,  6.2111e+04,  3.3223e+04]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sigmoid'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/root/artem_capstone/dlk.py\u001b[0m in \u001b[0;36mline 6\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=172'>173</a>\u001b[0m     res \u001b[39m=\u001b[39m reporter(cache[\u001b[39m47\u001b[39m]\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=173'>174</a>\u001b[0m     pp(res)\n\u001b[0;32m----> <a href='file:///root/artem_capstone/dlk.py?line=174'>175</a>\u001b[0m     res \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msigmoid()\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=175'>176</a>\u001b[0m pp(res\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='file:///root/artem_capstone/dlk.py?line=176'>177</a>\u001b[0m pp(dataset[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'sigmoid'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device))\n",
    "    res = res[0].sigmoid()\n",
    "pp(res.shape)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([85])\n",
      "[(22, 1), (42, 1), (63, 1), (85, 1)]\n",
      "22 1\n",
      "tensor(1., device='cuda:0')\n",
      "42 1\n",
      "tensor(1., device='cuda:0')\n",
      "63 1\n",
      "tensor(1., device='cuda:0')\n",
      "85 1\n",
      "tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device))\n",
    "    res = res[0]\n",
    "pp(res.shape)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([85])\n",
      "[(22, 1), (42, 1), (63, 1), (85, 1)]\n",
      "22 1\n",
      "tensor(66244.1094, device='cuda:0')\n",
      "42 1\n",
      "tensor(27276.3223, device='cuda:0')\n",
      "63 1\n",
      "tensor(41848.4961, device='cuda:0')\n",
      "85 1\n",
      "tensor(33222.6602, device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device))\n",
    "    res = res[0]\n",
    "pp(res.shape)\n",
    "pp(res)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([85])\n",
      "tensor([-3.1554e+04,  2.9691e+04,  4.8190e+04, -7.0022e+03,  3.0838e+04,\n",
      "         7.5812e+04, -1.6684e+04,  5.7554e+04,  5.1635e+04, -6.7735e+04,\n",
      "        -4.5031e+04,  2.9842e+04,  3.6086e+04,  1.5849e+04,  1.6806e+04,\n",
      "        -4.8002e+04,  4.2133e+04,  6.3501e+01,  1.2088e+05,  8.0037e+03,\n",
      "         5.9227e+04,  6.6244e+04,  3.0824e+04,  1.1024e+05,  2.6843e+04,\n",
      "        -2.0253e+03,  6.2194e+04,  6.2850e+03, -4.8561e+04,  3.5525e+04,\n",
      "         3.2681e+04, -7.6061e+04,  5.2919e+04,  9.0775e+04,  2.0617e+04,\n",
      "         2.8052e+03,  6.4431e+04,  1.5134e+04,  4.5953e+04, -1.9247e+04,\n",
      "         9.7630e+04,  2.7276e+04,  3.0647e+04,  4.2342e+04, -1.1101e+03,\n",
      "        -2.3795e+04,  1.6483e+05,  4.5880e+04,  6.5606e+04, -2.5157e+04,\n",
      "        -7.7115e+03,  9.4512e+04, -5.0809e+04,  7.8759e+04,  6.9595e+04,\n",
      "         2.9231e+03,  4.7988e+04,  3.6668e+03,  5.4307e+04,  3.1461e+04,\n",
      "        -4.5519e+03,  2.5983e+04,  4.1848e+04, -9.1882e+03,  1.9365e+04,\n",
      "        -5.7895e+03,  7.9265e+03,  4.5831e+04,  3.4234e+04, -4.4444e+03,\n",
      "         3.6480e+04, -5.6625e+04,  4.5559e+04, -5.6081e+04,  4.0064e+04,\n",
      "         3.1871e+04,  2.0469e+04,  3.7624e+04,  1.3394e+04,  1.0201e+05,\n",
      "         5.1215e+04,  1.5066e+04,  8.1638e+03,  6.2111e+04,  3.3223e+04],\n",
      "       device='cuda:0')\n",
      "[(22, 1), (42, 1), (63, 1), (85, 1)]\n",
      "22 1\n",
      "tensor(66244.1094, device='cuda:0')\n",
      "42 1\n",
      "tensor(27276.3223, device='cuda:0')\n",
      "63 1\n",
      "tensor(41848.4961, device='cuda:0')\n",
      "85 1\n",
      "tensor(33222.6602, device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device))\n",
    "    res = res[0]\n",
    "pp(res.shape)\n",
    "pp(res > 0)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([85])\n",
      "tensor([False,  True,  True, False,  True,  True, False,  True,  True, False,\n",
      "        False,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False,  True,  True, False,  True,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False, False,  True,  True,  True, False,\n",
      "        False,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True, False,  True, False,  True,  True,  True, False,\n",
      "         True, False,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True], device='cuda:0')\n",
      "[(22, 1), (42, 1), (63, 1), (85, 1)]\n",
      "22 1\n",
      "tensor(66244.1094, device='cuda:0')\n",
      "42 1\n",
      "tensor(27276.3223, device='cuda:0')\n",
      "63 1\n",
      "tensor(41848.4961, device='cuda:0')\n",
      "85 1\n",
      "tensor(33222.6602, device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "with torch.inference_mode():\n",
    "    #res = reporter(cache_true['mlp_out', 47][0]).sigmoid()\n",
    "    res = reporter(cache[47].to(device))\n",
    "    res = res[0]\n",
    "pp(res.shape)\n",
    "pp(res)\n",
    "pp(dataset[0][1])\n",
    "for inx, label in dataset[0][1]:\n",
    "    print(inx, label)\n",
    "    pp(res[inx-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([85])\n",
      "tensor([-3.1554e+04,  2.9691e+04,  4.8190e+04, -7.0022e+03,  3.0838e+04,\n",
      "         7.5812e+04, -1.6684e+04,  5.7554e+04,  5.1635e+04, -6.7735e+04,\n",
      "        -4.5031e+04,  2.9842e+04,  3.6086e+04,  1.5849e+04,  1.6806e+04,\n",
      "        -4.8002e+04,  4.2133e+04,  6.3501e+01,  1.2088e+05,  8.0037e+03,\n",
      "         5.9227e+04,  6.6244e+04,  3.0824e+04,  1.1024e+05,  2.6843e+04,\n",
      "        -2.0253e+03,  6.2194e+04,  6.2850e+03, -4.8561e+04,  3.5525e+04,\n",
      "         3.2681e+04, -7.6061e+04,  5.2919e+04,  9.0775e+04,  2.0617e+04,\n",
      "         2.8052e+03,  6.4431e+04,  1.5134e+04,  4.5953e+04, -1.9247e+04,\n",
      "         9.7630e+04,  2.7276e+04,  3.0647e+04,  4.2342e+04, -1.1101e+03,\n",
      "        -2.3795e+04,  1.6483e+05,  4.5880e+04,  6.5606e+04, -2.5157e+04,\n",
      "        -7.7115e+03,  9.4512e+04, -5.0809e+04,  7.8759e+04,  6.9595e+04,\n",
      "         2.9231e+03,  4.7988e+04,  3.6668e+03,  5.4307e+04,  3.1461e+04,\n",
      "        -4.5519e+03,  2.5983e+04,  4.1848e+04, -9.1882e+03,  1.9365e+04,\n",
      "        -5.7895e+03,  7.9265e+03,  4.5831e+04,  3.4234e+04, -4.4444e+03,\n",
      "         3.6480e+04, -5.6625e+04,  4.5559e+04, -5.6081e+04,  4.0064e+04,\n",
      "         3.1871e+04,  2.0469e+04,  3.7624e+04,  1.3394e+04,  1.0201e+05,\n",
      "         5.1215e+04,  1.5066e+04,  8.1638e+03,  6.2111e+04,  3.3223e+04],\n",
      "       device='cuda:0')\n",
      "[(22, 1), (42, 1), (63, 1), (85, 1)]\n",
      "22 1\n",
      "tensor(66244.1094, device='cuda:0')\n",
      "42 1\n",
      "tensor(27276.3223, device='cuda:0')\n",
      "63 1\n",
      "tensor(41848.4961, device='cuda:0')\n",
      "85 1\n",
      "tensor(33222.6602, device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}