{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from IPython.display import display\n",
    "from collections import namedtuple\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from jaxtyping import Int, Float\n",
    "from jinja2 import Environment, PackageLoader, select_autoescape\n",
    "from pathlib import Path\n",
    "from pprint import pp\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from utils.truthful_qa_ds import get_question_answer_dataset\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "login()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pp(device)\n",
    "np_rand = np.random.default_rng(seed=100500)\n",
    "model_type = torch.float16"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0f242acb9ba4fd2b0bb9f25171f3f96"
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device(type='cuda')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Load model\n",
    "# llama_path = \"../llama/7bf_converted/\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    torch_dtype=model_type,\n",
    "    device_map=device,\n",
    ")\n",
    "model.eval()\n",
    "pp(model)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1aeeff28e39749d581ff7e3df935d057"
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f76dd9ef91442ffadc11ccabd9d6106"
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92ce4983d76941bfbcbc4d5c5abd3fae"
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7bfa230788d4166934e9295adcc1fee"
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55ed9e03bd4b4e39a5b60731705ebbdd"
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5afca7835df474ba989774211ed9c9a"
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d782f320e6574246a861a18cb122563b"
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d9c822d57224c1aae8d5db702aefe4e"
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b25fd4fd4b848c68e88f4fd2684d844"
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97349f58901a46b3962e60c1965e0d7c"
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3859da657e3478fb67ebcffdb1cffc6"
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "true_token = tokenizer.encode(\"True\")[1]\n",
    "false_token = tokenizer.encode(\"False\")[1]\n",
    "print(true_token)\n",
    "print(false_token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5852\n",
      "7700\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "truthfulqa = load_dataset(\"truthful_qa\", \"generation\")  # 817 rows\n",
    "env = Environment(loader=PackageLoader(\"utils\"), autoescape=select_autoescape())"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3d825a5714d4eca8f798a038925d27e"
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02db462771b2494ca5fe1eeb4d9b1651"
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b41c5505c21443d98123375c181df73"
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87955dd03d0845ba947cc26faf048c8a"
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/99.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "909e55c8a9a54244ba5291f56de1cad8"
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Accuracy on the TruthfulQA dataset, few shot:\n",
    "count = 0\n",
    "correct_samples = []\n",
    "correct_n = 0\n",
    "qa_t = env.get_template(\"question_answer.jinja\")\n",
    "with torch.no_grad():\n",
    "    p_bar = tqdm(list(enumerate(truthfulqa[\"validation\"])))\n",
    "    for i, row in p_bar:\n",
    "        def is_correct_answer(take_correct):\n",
    "            input_ = qa_t.render(\n",
    "                row,\n",
    "                is_correct_answer=take_correct\n",
    "            ),\n",
    "            t_output = tokenizer(input_, return_tensors=\"pt\")\n",
    "            t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "            outputs = model(**t_output, output_hidden_states=False)\n",
    "            pred = outputs.logits[0, -1].softmax(dim=-1)\n",
    "            predicted = (pred[true_token] > pred[false_token]).item()\n",
    "            return predicted == take_correct\n",
    "        with_true = is_correct_answer(True)\n",
    "        count += 1\n",
    "        if with_true:\n",
    "            correct_n += 1\n",
    "        with_false = is_correct_answer(False)\n",
    "        count += 1\n",
    "        if with_false:\n",
    "            correct_n += 1\n",
    "        if with_true and with_false:\n",
    "            correct_samples.append(row)\n",
    "        p_bar.set_description(\n",
    "            f\"Correct {correct_n}, count {count}, accuracy {correct_n / count:.4}, both {len(correct_samples)}\"\n",
    "        )\n",
    "# Result: Correct 972, count 1634, accuracy 0.5949, both 261"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Correct 969, count 1634, accuracy 0.593, both 259: 100%|██████████| 817/817 [01:53<00:00,  7.21it/s] \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Accuracy on the TruthfulQA dataset, few shot:\n",
    "count = 0\n",
    "correct_samples = []\n",
    "correct_n = 0\n",
    "qa_t = env.get_template(\"question_answer.jinja\")\n",
    "with torch.no_grad():\n",
    "    p_bar = tqdm(list(enumerate(truthfulqa[\"validation\"])))\n",
    "    for i, row in p_bar:\n",
    "        def is_correct_answer(take_correct):\n",
    "            input_ = qa_t.render(\n",
    "                row,\n",
    "                is_correct_answer=take_correct\n",
    "            ),\n",
    "            t_output = tokenizer(input_, return_tensors=\"pt\")\n",
    "            t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "            outputs = model(**t_output, output_hidden_states=False)\n",
    "            pred = outputs.logits[0, -1].softmax(dim=-1)\n",
    "            predicted = (pred[true_token] > pred[false_token]).item()\n",
    "            return predicted == take_correct\n",
    "        with_true = is_correct_answer(True)\n",
    "        count += 1\n",
    "        if with_true:\n",
    "            correct_n += 1\n",
    "        with_false = is_correct_answer(False)\n",
    "        count += 1\n",
    "        if with_false:\n",
    "            correct_n += 1\n",
    "        if with_true and with_false:\n",
    "            correct_samples.append(row)\n",
    "        p_bar.set_description(\n",
    "            f\"Correct {correct_n}, count {count}, accuracy {correct_n / count:.4}, both {len(correct_samples)}\"\n",
    "        )\n",
    "# Result: Correct 972, count 1634, accuracy 0.5949, both 261"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Correct 950, count 1634, accuracy 0.5814, both 242: 100%|██████████| 817/817 [02:03<00:00,  6.62it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Now calculate accuracy for compound sentences for correctly \n",
    "# detected samples. Expectation: should guess all correctly.\n",
    "\n",
    "count = 0\n",
    "correct_compound = []\n",
    "correct_n = 0\n",
    "qas_t = env.get_template(\"question_answers.jinja\")\n",
    "with torch.no_grad():\n",
    "    p_bar = tqdm(list(enumerate(correct_samples)))\n",
    "    for i, row in p_bar:\n",
    "        def is_correct_answer(take_correct):\n",
    "            input_ = qas_t.render(\n",
    "                row,\n",
    "                is_correct_answer=take_correct,\n",
    "                is_disjunction=False,\n",
    "            ),\n",
    "            t_output = tokenizer(input_, return_tensors=\"pt\")\n",
    "            t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "            outputs = model(**t_output, output_hidden_states=False)\n",
    "            pred = outputs.logits[0, -1].softmax(dim=-1)\n",
    "            predicted = (pred[true_token] > pred[false_token]).item()\n",
    "            return predicted == take_correct\n",
    "        with_true = is_correct_answer(True)\n",
    "        count += 1\n",
    "        if with_true:\n",
    "            correct_n += 1\n",
    "        with_false = is_correct_answer(False)\n",
    "        count += 1\n",
    "        if with_false:\n",
    "            correct_n += 1\n",
    "        if with_true and with_false:\n",
    "            correct_compound.append(row)\n",
    "        p_bar.set_description(\n",
    "            f\"Correct {correct_n}, count {count}, accuracy {correct_n / count:.4}, both {len(correct_compound)}\"\n",
    "        )\n",
    "\n",
    "# Random? Correct 289, count 522, accuracy 0.5536, both 48: 100%|██████████| 261/261 [00:44<00:00,  5.84it/s]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Correct 261, count 484, accuracy 0.5393, both 37: 100%|██████████| 242/242 [00:53<00:00,  4.49it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Accuracy on the TruthfulQA dataset, few shot:\n",
    "count = 0\n",
    "correct_samples = []\n",
    "correct_n = 0\n",
    "qa_t = env.get_template(\"question_answer.jinja\")\n",
    "with torch.no_grad():\n",
    "    p_bar = tqdm(list(enumerate(truthfulqa[\"validation\"])))\n",
    "    for i, row in p_bar:\n",
    "        def is_correct_answer(take_correct):\n",
    "            input_ = qa_t.render(\n",
    "                row,\n",
    "                is_correct_answer=take_correct\n",
    "            ),\n",
    "            t_output = tokenizer(input_, return_tensors=\"pt\")\n",
    "            t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "            outputs = model(**t_output, output_hidden_states=False)\n",
    "            pred = outputs.logits[0, -1].softmax(dim=-1)\n",
    "            predicted = (pred[true_token] > pred[false_token]).item()\n",
    "            return predicted == take_correct\n",
    "        with_true = is_correct_answer(True)\n",
    "        count += 1\n",
    "        if with_true:\n",
    "            correct_n += 1\n",
    "        with_false = is_correct_answer(False)\n",
    "        count += 1\n",
    "        if with_false:\n",
    "            correct_n += 1\n",
    "        if with_true and with_false:\n",
    "            correct_samples.append(row)\n",
    "        p_bar.set_description(\n",
    "            f\"Correct {correct_n}, count {count}, accuracy {correct_n / count:.4}, both {len(correct_samples)}\"\n",
    "        )\n",
    "# Result: Correct 972, count 1634, accuracy 0.5949, both 261"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Correct 969, count 1634, accuracy 0.593, both 259: 100%|██████████| 817/817 [01:53<00:00,  7.22it/s] \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Now calculate accuracy for compound sentences for correctly \n",
    "# detected samples. Expectation: should guess all correctly.\n",
    "\n",
    "count = 0\n",
    "correct_compound = []\n",
    "correct_n = 0\n",
    "qas_t = env.get_template(\"question_answers.jinja\")\n",
    "with torch.no_grad():\n",
    "    p_bar = tqdm(list(enumerate(correct_samples)))\n",
    "    for i, row in p_bar:\n",
    "        def is_correct_answer(take_correct):\n",
    "            input_ = qas_t.render(\n",
    "                row,\n",
    "                is_correct_answer=take_correct,\n",
    "                is_disjunction=False,\n",
    "            ),\n",
    "            t_output = tokenizer(input_, return_tensors=\"pt\")\n",
    "            t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "            outputs = model(**t_output, output_hidden_states=False)\n",
    "            pred = outputs.logits[0, -1].softmax(dim=-1)\n",
    "            predicted = (pred[true_token] > pred[false_token]).item()\n",
    "            return predicted == take_correct\n",
    "        with_true = is_correct_answer(True)\n",
    "        count += 1\n",
    "        if with_true:\n",
    "            correct_n += 1\n",
    "        with_false = is_correct_answer(False)\n",
    "        count += 1\n",
    "        if with_false:\n",
    "            correct_n += 1\n",
    "        if with_true and with_false:\n",
    "            correct_compound.append(row)\n",
    "        p_bar.set_description(\n",
    "            f\"Correct {correct_n}, count {count}, accuracy {correct_n / count:.4}, both {len(correct_compound)}\"\n",
    "        )\n",
    "\n",
    "# Random? Correct 289, count 522, accuracy 0.5536, both 48: 100%|██████████| 261/261 [00:44<00:00,  5.84it/s]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Correct 287, count 518, accuracy 0.5541, both 48: 100%|██████████| 259/259 [00:55<00:00,  4.64it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Previous 2 accuracy cells without: \"Answer these questions:\". It is 1.8 percent higher\n",
    "# "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def get_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    \n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def format_imdb(text, label):\n",
    "    \"\"\"\n",
    "    Given an imdb example (\"text\") and corresponding label (0 for negative, or 1 for positive), \n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "    \n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"The following movie review expresses a \" + [\"negative\", \"positive\"][label] + \" sentiment:\\n\" + text\n",
    "\n",
    "\n",
    "def get_hidden_states_many_examples(model, tokenizer, data, model_type, n=100):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples.\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    \n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "\n",
    "    # loop\n",
    "    for _ in tqdm(range(n)):\n",
    "        # for simplicity, sample a random example until we find one that's a reasonable length\n",
    "        # (most examples should be a reasonable length, so this is just to make sure)\n",
    "        while True:\n",
    "            idx = np.random.randint(len(data))\n",
    "            text, true_label = data[idx][\"content\"], data[idx][\"label\"]\n",
    "            # the actual formatted input will be longer, so include a bit of a marign\n",
    "            if len(tokenizer(text)) < 400:  \n",
    "                break\n",
    "                \n",
    "        # get hidden states\n",
    "        neg_hs = get_hidden_states(model, tokenizer, format_imdb(text, 0), model_type=model_type)\n",
    "        pos_hs = get_hidden_states(model, tokenizer, format_imdb(text, 1), model_type=model_type)\n",
    "\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "        all_gt_labels.append(true_label)\n",
    "\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "    all_gt_labels = np.stack(all_gt_labels)\n",
    "\n",
    "    return all_neg_hs, all_pos_hs, all_gt_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    ""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def get_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    \n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def format_row(row, label, true_label):\n",
    "    qa_t = env.get_template(\"question_answer.jinja\")\n",
    "    return qa_t.render(\n",
    "        row,\n",
    "        is_correct_answer=true_label,\n",
    "        label=label,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_hidden_states_many_examples(model, tokenizer, data, model_type, n=100):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples.\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    \n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "\n",
    "    # loop\n",
    "    for i in tqdm(range(n)):\n",
    "        true_label = i % 2 == 0\n",
    "        # get hidden states\n",
    "        neg_hs = get_hidden_states(model, tokenizer, format_row(data[i], True, true_label), model_type=model_type)\n",
    "        pos_hs = get_hidden_states(model, tokenizer, format_row(data[i], False, true_label), model_type=model_type)\n",
    "\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "        all_gt_labels.append(true_label)\n",
    "\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "    all_gt_labels = np.stack(all_gt_labels)\n",
    "\n",
    "    return all_neg_hs, all_pos_hs, all_gt_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, truthfulqa[\"validation\"], model_type)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "get_hidden_states() got an unexpected keyword argument 'model_type'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/dlkworks/llama.py\u001b[0m in \u001b[0;36mline 2\n\u001b[1;32m      <a href='file:///workspace/dlkworks/llama.py?line=256'>257</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[0;32m----> <a href='file:///workspace/dlkworks/llama.py?line=257'>258</a>\u001b[0m neg_hs, pos_hs, y \u001b[39m=\u001b[39m get_hidden_states_many_examples(model, tokenizer, truthfulqa[\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m], model_type)\n",
      "\u001b[1;32m/workspace/dlkworks/llama.py\u001b[0m in \u001b[0;36mline 48\u001b[0m, in \u001b[0;36mget_hidden_states_many_examples\u001b[0;34m(model, tokenizer, data, model_type, n)\n\u001b[1;32m     <a href='file:///workspace/dlkworks/llama.py?line=240'>241</a>\u001b[0m true_label \u001b[39m=\u001b[39m i \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='file:///workspace/dlkworks/llama.py?line=241'>242</a>\u001b[0m \u001b[39m# get hidden states\u001b[39;00m\n\u001b[0;32m---> <a href='file:///workspace/dlkworks/llama.py?line=242'>243</a>\u001b[0m neg_hs \u001b[39m=\u001b[39m get_hidden_states(model, tokenizer, format_row(data[i], \u001b[39mTrue\u001b[39;49;00m, true_label), model_type\u001b[39m=\u001b[39;49mmodel_type)\n\u001b[1;32m     <a href='file:///workspace/dlkworks/llama.py?line=243'>244</a>\u001b[0m pos_hs \u001b[39m=\u001b[39m get_hidden_states(model, tokenizer, format_row(data[i], \u001b[39mFalse\u001b[39;00m, true_label), model_type\u001b[39m=\u001b[39mmodel_type)\n\u001b[1;32m     <a href='file:///workspace/dlkworks/llama.py?line=245'>246</a>\u001b[0m \u001b[39m# collect\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_hidden_states() got an unexpected keyword argument 'model_type'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def get_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    \n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def format_row(row, label, true_label):\n",
    "    qa_t = env.get_template(\"question_answer.jinja\")\n",
    "    return qa_t.render(\n",
    "        row,\n",
    "        is_correct_answer=true_label,\n",
    "        label=label,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_hidden_states_many_examples(model, tokenizer, data, n=100):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples.\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    \n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "\n",
    "    # loop\n",
    "    for i in tqdm(range(n)):\n",
    "        true_label = i % 2 == 0\n",
    "        # get hidden states\n",
    "        neg_hs = get_hidden_states(model, tokenizer, format_row(data[i], True, true_label))\n",
    "        pos_hs = get_hidden_states(model, tokenizer, format_row(data[i], False, true_label))\n",
    "\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "        all_gt_labels.append(true_label)\n",
    "\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "    all_gt_labels = np.stack(all_gt_labels)\n",
    "\n",
    "    return all_neg_hs, all_pos_hs, all_gt_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, truthfulqa[\"validation\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 100/100 [00:14<00:00,  7.07it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# let's create a simple 50/50 train split (the data is already randomized)\n",
    "n = len(y)\n",
    "neg_hs_train, neg_hs_test = neg_hs[:n//2], neg_hs[n//2:]\n",
    "pos_hs_train, pos_hs_test = pos_hs[:n//2], pos_hs[n//2:]\n",
    "y_train, y_test = y[:n//2], y[n//2:]\n",
    "\n",
    "# for simplicity we can just take the difference between positive and negative hidden states\n",
    "# (concatenating also works fine)\n",
    "x_train = neg_hs_train - pos_hs_train\n",
    "x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(x_train, y_train)\n",
    "print(\"Logistic regression accuracy: {}\".format(lr.score(x_test, y_test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logistic regression accuracy: 0.84\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class MLPProbe(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d, 100)\n",
    "        self.linear2 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.linear1(x))\n",
    "        o = self.linear2(h)\n",
    "        return torch.sigmoid(o)\n",
    "\n",
    "class CCS(object):\n",
    "    def __init__(self, x0, x1, nepochs=1000, ntries=10, lr=1e-3, batch_size=-1, \n",
    "                 verbose=False, device=\"cuda\", linear=True, weight_decay=0.01, var_normalize=False):\n",
    "        # data\n",
    "        self.var_normalize = var_normalize\n",
    "        self.x0 = self.normalize(x0)\n",
    "        self.x1 = self.normalize(x1)\n",
    "        self.d = self.x0.shape[-1]\n",
    "\n",
    "        # training\n",
    "        self.nepochs = nepochs\n",
    "        self.ntries = ntries\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # probe\n",
    "        self.linear = linear\n",
    "        self.initialize_probe()\n",
    "        self.best_probe = copy.deepcopy(self.probe)\n",
    "\n",
    "        \n",
    "    def initialize_probe(self):\n",
    "        if self.linear:\n",
    "            self.probe = nn.Sequential(nn.Linear(self.d, 1), nn.Sigmoid())\n",
    "        else:\n",
    "            self.probe = MLPProbe(self.d)\n",
    "        self.probe.to(self.device)    \n",
    "\n",
    "\n",
    "    def normalize(self, x):\n",
    "        \"\"\"\n",
    "        Mean-normalizes the data x (of shape (n, d))\n",
    "        If self.var_normalize, also divides by the standard deviation\n",
    "        \"\"\"\n",
    "        normalized_x = x - x.mean(axis=0, keepdims=True)\n",
    "        if self.var_normalize:\n",
    "            normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
    "\n",
    "        return normalized_x\n",
    "\n",
    "        \n",
    "    def get_tensor_data(self):\n",
    "        \"\"\"\n",
    "        Returns x0, x1 as appropriate tensors (rather than np arrays)\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.x0, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.x1, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        return x0, x1\n",
    "    \n",
    "\n",
    "    def get_loss(self, p0, p1):\n",
    "        \"\"\"\n",
    "        Returns the CCS loss for two probabilities each of shape (n,1) or (n,)\n",
    "        \"\"\"\n",
    "        informative_loss = (torch.min(p0, p1)**2).mean(0)\n",
    "        consistent_loss = ((p0 - (1-p1))**2).mean(0)\n",
    "        return informative_loss + consistent_loss\n",
    "\n",
    "\n",
    "    def get_acc(self, x0_test, x1_test, y_test):\n",
    "        \"\"\"\n",
    "        Computes accuracy for the current parameters on the given test inputs\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.normalize(x0_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.normalize(x1_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            p0, p1 = self.best_probe(x0), self.best_probe(x1)\n",
    "        avg_confidence = 0.5*(p0 + (1-p1))\n",
    "        predictions = (avg_confidence.detach().cpu().numpy() < 0.5).astype(int)[:, 0]\n",
    "        acc = (predictions == y_test).mean()\n",
    "        acc = max(acc, 1 - acc)\n",
    "\n",
    "        return acc\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Does a single training run of nepochs epochs\n",
    "        \"\"\"\n",
    "        x0, x1 = self.get_tensor_data()\n",
    "        permutation = torch.randperm(len(x0))\n",
    "        x0, x1 = x0[permutation], x1[permutation]\n",
    "        \n",
    "        # set up optimizer\n",
    "        optimizer = torch.optim.AdamW(self.probe.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "        batch_size = len(x0) if self.batch_size == -1 else self.batch_size\n",
    "        nbatches = len(x0) // batch_size\n",
    "\n",
    "        # Start training (full batch)\n",
    "        for epoch in range(self.nepochs):\n",
    "            for j in range(nbatches):\n",
    "                x0_batch = x0[j*batch_size:(j+1)*batch_size]\n",
    "                x1_batch = x1[j*batch_size:(j+1)*batch_size]\n",
    "            \n",
    "                # probe\n",
    "                p0, p1 = self.probe(x0_batch), self.probe(x1_batch)\n",
    "\n",
    "                # get the corresponding loss\n",
    "                loss = self.get_loss(p0, p1)\n",
    "\n",
    "                # update the parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return loss.detach().cpu().item()\n",
    "    \n",
    "    def repeated_train(self):\n",
    "        best_loss = np.inf\n",
    "        for train_num in range(self.ntries):\n",
    "            self.initialize_probe()\n",
    "            loss = self.train()\n",
    "            if loss < best_loss:\n",
    "                self.best_probe = copy.deepcopy(self.probe)\n",
    "                best_loss = loss\n",
    "\n",
    "        return best_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Train CCS without any labels\n",
    "ccs = CCS(neg_hs_train, pos_hs_train)\n",
    "ccs.repeated_train()\n",
    "\n",
    "# Evaluate\n",
    "ccs_acc = ccs.get_acc(neg_hs_test, pos_hs_test, y_test)\n",
    "print(\"CCS accuracy: {}\".format(ccs_acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CCS accuracy: 0.82\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}