{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from IPython.display import display\n",
    "from collections import namedtuple\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from jaxtyping import Int, Float\n",
    "from jinja2 import Environment, PackageLoader, select_autoescape\n",
    "from pathlib import Path\n",
    "from pprint import pp\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from utils.truthful_qa_ds import get_question_answer_dataset\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "login()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pp(device)\n",
    "np_rand = np.random.default_rng(seed=100500)\n",
    "model_type = torch.float16"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "115bd3b8c0e24fe094d04d0f51f909f9"
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device(type='cuda')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Load model\n",
    "# llama_path = \"../llama/7bf_converted/\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    torch_dtype=model_type,\n",
    "    device_map=device,\n",
    ")\n",
    "model.eval()\n",
    "pp(model)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08dffe58314a49cea60408842e6d45a2"
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "true_token = tokenizer.encode(\"True\")[1]\n",
    "false_token = tokenizer.encode(\"False\")[1]\n",
    "print(true_token)\n",
    "print(false_token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5852\n",
      "7700\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "truthfulqa = load_dataset(\"truthful_qa\", \"generation\")  # 817 rows\n",
    "env = Environment(loader=PackageLoader(\"utils\"), autoescape=select_autoescape())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Playing with Tokenizer:"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# qa_t = env.get_template(\"question_answer.jinja\")\n",
    "# qa_dataset = []\n",
    "# for i, row in enumerate(truthfulqa[\"validation\"]):\n",
    "#     if len(row[\"correct_answers\"]) < 2:\n",
    "#         continue\n",
    "#     take_correct = i % 2 == 0\n",
    "#     for label in (True, False):\n",
    "#         qa_dataset.append(\n",
    "#             {\n",
    "#                 \"input\": qa_t.render(\n",
    "#                     row,\n",
    "#                     is_correct_answer=take_correct,\n",
    "#                     label=label,\n",
    "#                 ),\n",
    "#                 \"label\": label,\n",
    "#                 \"is_correct\": take_correct,\n",
    "#             }\n",
    "#         )\n",
    "#         correct_a = row[\"correct_answers\"][0]\n",
    "#         second_correct_a = row[\"correct_answers\"][1]\n",
    "#         incorrect_a = row[\"incorrect_answers\"][0]\n",
    "#         # qas_and_dataset.append(\n",
    "#         #    {\n",
    "#         #        \"input\": qas_t.render(\n",
    "#         #            row,\n",
    "#         #            a_A=correct_a if take_correct else incorrect_a,\n",
    "#         #            a_B=second_correct_a,\n",
    "#         #            is_disjunction=False,\n",
    "#         #            label=label,\n",
    "#         #        ),\n",
    "#         #        \"label\": label,\n",
    "#         #        \"is_correct\": take_correct,\n",
    "#         #    }\n",
    "#         # )\n",
    "# pp(qa_dataset[0])\n",
    "#\n",
    "# # %%\n",
    "# t_output = tokenizer(qa_dataset[0][\"input\"], return_tensors=\"pt\")\n",
    "#\n",
    "# # %%\n",
    "# pp(t_output)\n",
    "# pp(len(t_output))\n",
    "# pp(tokenizer.convert_ids_to_tokens(t_output[\"input_ids\"][0, -1].item()))\n",
    "# pp(tokenizer.convert_ids_to_tokens(true_token))\n",
    "# pp(tokenizer.convert_ids_to_tokens(false_token))\n",
    "#\n",
    "# # %%\n",
    "# pp(qa_dataset[0])\n",
    "# pp(qa_dataset[1])\n",
    "#\n",
    "# # %%\n",
    "# t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "# outputs = model(**t_output, output_hidden_states=True)\n",
    "# # %%\n",
    "# pred = outputs.logits[0, -2].softmax(dim=-1)\n",
    "# pp(pred)\n",
    "#\n",
    "# # %%\n",
    "# pp(\n",
    "#     f\"Probability of the last outputed token: {pred[t_output['input_ids'][0, -1].item()]}\"\n",
    "# )\n",
    "# pp(f\"True token probability: {pred[true_token]}\")\n",
    "# pp(f\"False token probability: {pred[false_token]}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def get_samples(template):\n",
    "    qa_t = env.get_template(template)\n",
    "    result = []\n",
    "    for i, row in enumerate(truthfulqa[\"validation\"]):\n",
    "        if len(row[\"correct_answers\"]) < 2:\n",
    "            continue\n",
    "        take_correct = i % 2 == 0\n",
    "        for label in (True, False):\n",
    "            result.append(\n",
    "                {\n",
    "                    \"index\": i,\n",
    "                    \"row\": row,\n",
    "                    \"input\": qa_t.render(\n",
    "                        row,\n",
    "                        is_correct_answer=take_correct,\n",
    "                        label=label,\n",
    "                    ),\n",
    "                    \"label\": label,\n",
    "                    \"is_correct\": take_correct,\n",
    "                }\n",
    "            )\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Accuracy on the TruthfulQA dataset:\n",
    "count = 0\n",
    "true_count = 0\n",
    "correct_samples = []\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(get_samples(\"question_answer.jinja\")[:1000]):\n",
    "        count += 1\n",
    "        t_output = tokenizer(sample[\"input\"], return_tensors=\"pt\")\n",
    "        t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "        outputs = model(**t_output, output_hidden_states=False)\n",
    "        pred = outputs.logits[0, -2].softmax(dim=-1)\n",
    "        true_prob = pred[true_token]\n",
    "        false_prob = pred[false_token]\n",
    "        is_true = true_prob > false_prob\n",
    "        true_count += int(sample[\"label\"])\n",
    "        if is_true == sample[\"is_correct\"]:\n",
    "            correct_samples.append(sample[\"row\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1000/1000 [00:57<00:00, 17.28it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "correct_n = len(correct_samples)\n",
    "print(\n",
    "    f\"Correct {correct_n}, count {count}, accuracy {correct_n / count:.4}, true label count {true_count}\"\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Correct 602, count 1000, accuracy 0.602, true label count 500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Calculate accuracy for correctly detectedly samples.\n",
    "qas_correct_samples = []\n",
    "qas_t = env.get_template(\"question_answers.jinja\")\n",
    "for i, row in tqdm(enumerate(correct_samples)):\n",
    "    take_correct = i % 2 == 0\n",
    "    for label in (True, False):\n",
    "        input_ = (\n",
    "            qas_t.render(\n",
    "                row=row,\n",
    "                is_disjunction=True,\n",
    "                is_correct_answer=take_correct,\n",
    "                label=label,\n",
    "            ),\n",
    "        )\n",
    "        t_output = tokenizer(input_, return_tensors=\"pt\")\n",
    "        t_output = {k: t_output[k].to(device) for k in t_output}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**t_output, output_hidden_states=False)\n",
    "        pred = outputs.logits[0, -2].softmax(dim=-1)\n",
    "        true_prob = pred[true_token]\n",
    "        false_prob = pred[false_token]\n",
    "        is_true = true_prob > false_prob\n",
    "        true_count += int(label)\n",
    "        if is_true == take_correct:\n",
    "            qas_correct_samples.append((row, input_))\n",
    "\n",
    "correct_n = len(qas_correct_samples)\n",
    "print(\n",
    "    f\"Correct {correct_n}, count {count}, accuracy {correct_n / count:.4}, true label count {true_count}\"\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "602it [01:32,  6.50it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Correct 606, count 1000, accuracy 0.606, true label count 1102\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}